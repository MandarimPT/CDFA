{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample size = 900\n",
      "Using all features\n",
      "Testing sample size = 50\n",
      "Using all features\n",
      "Validation sample size = 50\n",
      "Using all features\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.000010.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.690383\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.650265\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.701628\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.702796\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.627193\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.703314\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 0.726081\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.699048\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.681299\n",
      "\n",
      "Test set: Average loss: 0.06942662843658567, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.614313\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.700117\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.715495\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.716700\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.650614\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.694937\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.718990\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.717668\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.705206\n",
      "\n",
      "Test set: Average loss: 0.06942862164119785, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.689477\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 0.671159\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.700972\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.659121\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.653064\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.686893\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.685892\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.660221\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.665593\n",
      "\n",
      "Test set: Average loss: 0.06942829917934754, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.725149\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.687731\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.690305\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.667528\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.662850\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.720000\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.643524\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.684143\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.614673\n",
      "\n",
      "Test set: Average loss: 0.06942872417091396, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.680418\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.667539\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.610439\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.736214\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.629177\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.721628\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.677971\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.823951\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.756822\n",
      "\n",
      "Test set: Average loss: 0.06943007406658493, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.668029\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.730330\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.748022\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.681399\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.687322\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.659714\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.640257\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.750099\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.629733\n",
      "\n",
      "Test set: Average loss: 0.0694314021663856, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.679626\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.719395\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.704051\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.710655\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.657144\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.696738\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.722610\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.713424\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.698481\n",
      "\n",
      "Test set: Average loss: 0.06943039127932481, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.731469\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.668260\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.734142\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.663910\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.703949\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.702479\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.680985\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.695374\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.670397\n",
      "\n",
      "Test set: Average loss: 0.06943163729917477, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.676706\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.623677\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.706444\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.675853\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.682813\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.666267\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.737427\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.721919\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.713529\n",
      "\n",
      "Test set: Average loss: 0.06943237950055167, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.676919\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.749726\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.631554\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.698928\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.689157\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.694726\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.653568\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.704689\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.624613\n",
      "\n",
      "Test set: Average loss: 0.06943275209827136, Accuracy: 56.00000000000001%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.000100.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.674294\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.721683\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.757391\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.667922\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.705339\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.720818\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 0.699493\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.675069\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.719270\n",
      "\n",
      "Test set: Average loss: 0.07003475318831054, Accuracy: 46.0%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.691252\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.797505\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.700269\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.711514\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.720964\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.728144\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.719108\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.688385\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.673913\n",
      "\n",
      "Test set: Average loss: 0.07006811272342213, Accuracy: 46.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.648393\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 0.753147\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.720994\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.679035\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.717934\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.700711\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.654863\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.664231\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.697048\n",
      "\n",
      "Test set: Average loss: 0.07011851391696816, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.694726\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.701550\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.775781\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.721736\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.656991\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.635908\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.709626\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.714737\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.657911\n",
      "\n",
      "Test set: Average loss: 0.07017277446798076, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.666043\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.641152\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.673077\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.725157\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.702913\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.673783\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.741255\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.691101\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.704088\n",
      "\n",
      "Test set: Average loss: 0.07019486057811422, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.654053\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.683351\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.646941\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.678385\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.770389\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.740020\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.682274\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.701171\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.679145\n",
      "\n",
      "Test set: Average loss: 0.07024236401555149, Accuracy: 44.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.707854\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.694112\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.694644\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.699303\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.740474\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.675567\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.706710\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.721598\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.658767\n",
      "\n",
      "Test set: Average loss: 0.07027443408195412, Accuracy: 44.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.707210\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.635338\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.694222\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.704266\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.644904\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.652042\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.673513\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.724992\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.710055\n",
      "\n",
      "Test set: Average loss: 0.07031557467439285, Accuracy: 38.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.686222\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.638949\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.790428\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.651133\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.752557\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.676826\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.644737\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.688904\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.705571\n",
      "\n",
      "Test set: Average loss: 0.07034837160313179, Accuracy: 38.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.709282\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.673099\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.661298\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.663793\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.660505\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.719187\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.616551\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.740818\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.710653\n",
      "\n",
      "Test set: Average loss: 0.0703854298664483, Accuracy: 38.0%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.001000.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.616686\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.732849\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.650405\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.716127\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.675172\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.710330\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 0.690824\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.659865\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.684572\n",
      "\n",
      "Test set: Average loss: 0.06964222757437342, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.715913\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.632206\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.709567\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.696225\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.725270\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.729108\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.804120\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.680931\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.715965\n",
      "\n",
      "Test set: Average loss: 0.06983401605939812, Accuracy: 52.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.667057\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 0.594965\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.725864\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.707760\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.679843\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.764791\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.686143\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.624669\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.719572\n",
      "\n",
      "Test set: Average loss: 0.07032903369189009, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.779253\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.676698\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.699841\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.711578\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.688371\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.689197\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.735343\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.724627\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.663899\n",
      "\n",
      "Test set: Average loss: 0.07041127946719118, Accuracy: 56.00000000000001%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.650213\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.755752\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.676289\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.663853\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.692901\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.732745\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.687469\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.722797\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.660198\n",
      "\n",
      "Test set: Average loss: 0.07023166787775072, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.728096\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.626401\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.662573\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.688024\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.690367\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.680636\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.735801\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.641030\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.621884\n",
      "\n",
      "Test set: Average loss: 0.0704809960473417, Accuracy: 54.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.661870\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.615412\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.670779\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.725947\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.636233\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.610606\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.604656\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.626608\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.691410\n",
      "\n",
      "Test set: Average loss: 0.0704714221822091, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.619356\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.626760\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.729425\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.724242\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.681109\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.699773\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.754535\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.658345\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.747027\n",
      "\n",
      "Test set: Average loss: 0.07032201832583046, Accuracy: 46.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.746143\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.678349\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.685993\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.767842\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.690132\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.571995\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.692326\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.657981\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.699706\n",
      "\n",
      "Test set: Average loss: 0.07046769264619102, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.712773\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.667499\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.702736\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.610052\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.693876\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.646058\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.634089\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.667709\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.642218\n",
      "\n",
      "Test set: Average loss: 0.07055423187685518, Accuracy: 46.0%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.010000.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.721652\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.732168\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.621955\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.682436\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.663775\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.683516\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 0.624492\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.567418\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.621338\n",
      "\n",
      "Test set: Average loss: 0.07163185069844649, Accuracy: 38.0%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.762741\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.674164\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.803746\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.717630\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.667635\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.618155\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.681633\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.789489\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.690320\n",
      "\n",
      "Test set: Average loss: 0.07184195713579151, Accuracy: 46.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.645612\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 0.649017\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.630118\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.673164\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.660402\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.735559\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.659567\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.772928\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.703213\n",
      "\n",
      "Test set: Average loss: 0.07135242637582823, Accuracy: 44.0%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.702025\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.702064\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.712538\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.724231\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.587590\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.666535\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.559084\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.592805\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.701334\n",
      "\n",
      "Test set: Average loss: 0.07435137484177436, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.758473\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.595850\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.636973\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.630969\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.783031\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.706313\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.701171\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.662746\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.678282\n",
      "\n",
      "Test set: Average loss: 0.07285313199593949, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.700915\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.781302\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.692853\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.584630\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.790464\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.700693\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.725251\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.681701\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.592614\n",
      "\n",
      "Test set: Average loss: 0.07281425811869242, Accuracy: 52.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.804949\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.621454\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.606745\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.659373\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.658099\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.627927\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.588787\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.618759\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.633040\n",
      "\n",
      "Test set: Average loss: 0.07124773460154989, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.773426\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.701554\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.570250\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.602155\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.789431\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.597694\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.724188\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.562074\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.677008\n",
      "\n",
      "Test set: Average loss: 0.07201716699269475, Accuracy: 42.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.676224\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.615324\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.519596\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.630136\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.709231\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.562025\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.574426\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.592901\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.622388\n",
      "\n",
      "Test set: Average loss: 0.07219867079938863, Accuracy: 44.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.609487\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.620013\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.749677\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.610117\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.556522\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.712075\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.607942\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.649569\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.717438\n",
      "\n",
      "Test set: Average loss: 0.07090346381327273, Accuracy: 46.0%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.100000.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.708522\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.851737\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 1.054605\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.627303\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.703546\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.779135\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 0.886312\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.938195\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.924860\n",
      "\n",
      "Test set: Average loss: 0.08181947418489294, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.602921\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.641765\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.636249\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.425339\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.681958\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.960124\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.710659\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.459924\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.916029\n",
      "\n",
      "Test set: Average loss: 0.07857342007420899, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.677853\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 0.757638\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.678593\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.739004\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.870643\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.578907\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.661645\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.717538\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 1.070763\n",
      "\n",
      "Test set: Average loss: 0.07695926173827389, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.959504\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.524518\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.666795\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.564893\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.784919\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.592965\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.715397\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.867507\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.901202\n",
      "\n",
      "Test set: Average loss: 0.07919136818090361, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.742992\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.604314\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.578238\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.792710\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.570933\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.791727\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.640443\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.772691\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.773673\n",
      "\n",
      "Test set: Average loss: 0.07531248394345509, Accuracy: 52.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.613181\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.618082\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.491799\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.522692\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.602386\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.499136\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.707274\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.760130\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.762026\n",
      "\n",
      "Test set: Average loss: 0.07348642067477489, Accuracy: 52.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.518193\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.735186\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.727208\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.603566\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.601438\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.525819\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.758060\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.346640\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.594962\n",
      "\n",
      "Test set: Average loss: 0.08363160131783914, Accuracy: 46.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.652822\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.834304\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.561473\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.494902\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.807962\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.517605\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.696151\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.582153\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.748134\n",
      "\n",
      "Test set: Average loss: 0.07497475378425476, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.581948\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.601723\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.843694\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.602930\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.477730\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.678879\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.758903\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.528039\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.922505\n",
      "\n",
      "Test set: Average loss: 0.07882825877268446, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.732162\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.586538\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.456357\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.557080\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.572914\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.717953\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.587888\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.771941\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.512985\n",
      "\n",
      "Test set: Average loss: 0.07977819730828037, Accuracy: 48.0%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.200000.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.696487\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.713094\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.911923\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.813236\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 1.089559\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.719248\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 1.014043\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.740145\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.681814\n",
      "\n",
      "Test set: Average loss: 0.08130466167444075, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.907076\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.623450\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.619493\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.860582\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.553188\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.714529\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.913247\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.813407\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.662801\n",
      "\n",
      "Test set: Average loss: 0.08132616875162083, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.942460\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 0.913262\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.613262\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.713262\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.713294\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.913262\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.813270\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182219, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.713262\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.913249\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 1.112965\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 1.112405\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.413262\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.713765\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 1.013261\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.813024\n",
      "\n",
      "Test set: Average loss: 0.0813261687518123, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.614089\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.713262\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 1.104201\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.613262\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.813272\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.413262\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.613264\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.713189\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182226, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.713234\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.913262\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.907397\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.613262\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.613262\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.613262\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.713254\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 1.013045\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.0813261687518134, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.613262\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.613262\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.713262\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.685930\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.513665\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.619392\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.712269\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 1.011974\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 1.108981\n",
      "\n",
      "Test set: Average loss: 0.0813261687518313, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.910897\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.578085\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.695691\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.713260\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.813265\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.613262\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.911117\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.712650\n",
      "\n",
      "Test set: Average loss: 0.08132616875188975, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.913896\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.713262\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.912902\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.742620\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.513262\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.713262\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182253, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.713262\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.651249\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.913093\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.713258\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 1.013262\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.513265\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.513262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182241, Accuracy: 50.0%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=0.500000.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.696617\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.912689\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.804286\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.711060\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.813276\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.513455\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 1.013184\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.804341\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.626588\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.713249\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.513305\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.513262\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.814980\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.613279\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.513283\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.913261\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.783436\n",
      "\n",
      "Test set: Average loss: 0.08281118487206768, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 1.013289\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 1.113262\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.846401\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.813261\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.613262\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 1.013262\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.082629733062441, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 1.013262\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.802538\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 1.013262\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.811477\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.718635\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 0.913262\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.913262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 1.013262\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.613262\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.713189\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.713262\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 0.713262\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.513262\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.913103\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.913262\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.913262\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 1.013262\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.513262\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182229, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.713262\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.713262\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.752482\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.913262\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.813262\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 0.713262\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.613262\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.830955\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.913280\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 1.113262\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 0.413262\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 0.713262\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.713262\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.813262\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.913262\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.713262\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.713262\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 1.113262\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.713262\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.713262\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 1.113262\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875180343, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.613262\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 1.013262\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.813262\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.713262\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.513262\n",
      "\n",
      "Test set: Average loss: 0.08132616875180343, Accuracy: 50.0%\n",
      "\n",
      "\n",
      "Training DNN with  1000 data points and SGD lr=1.000000.\n",
      "\n",
      "Train Epoch: 0 [0/900 (0%)]\tLoss: 0.666191\n",
      "Train Epoch: 0 [100/900 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [200/900 (22%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [300/900 (33%)]\tLoss: 0.913262\n",
      "Train Epoch: 0 [400/900 (44%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [500/900 (56%)]\tLoss: 0.713262\n",
      "Train Epoch: 0 [600/900 (67%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 0 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 1 [0/900 (0%)]\tLoss: 0.713262\n",
      "Train Epoch: 1 [100/900 (11%)]\tLoss: 0.913262\n",
      "Train Epoch: 1 [200/900 (22%)]\tLoss: 0.313262\n",
      "Train Epoch: 1 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [400/900 (44%)]\tLoss: 0.713262\n",
      "Train Epoch: 1 [500/900 (56%)]\tLoss: 0.913262\n",
      "Train Epoch: 1 [600/900 (67%)]\tLoss: 0.613262\n",
      "Train Epoch: 1 [700/900 (78%)]\tLoss: 0.813262\n",
      "Train Epoch: 1 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 2 [0/900 (0%)]\tLoss: 0.713262\n",
      "Train Epoch: 2 [100/900 (11%)]\tLoss: 1.013262\n",
      "Train Epoch: 2 [200/900 (22%)]\tLoss: 0.713262\n",
      "Train Epoch: 2 [300/900 (33%)]\tLoss: 0.513262\n",
      "Train Epoch: 2 [400/900 (44%)]\tLoss: 0.613262\n",
      "Train Epoch: 2 [500/900 (56%)]\tLoss: 0.913262\n",
      "Train Epoch: 2 [600/900 (67%)]\tLoss: 0.813262\n",
      "Train Epoch: 2 [700/900 (78%)]\tLoss: 1.113262\n",
      "Train Epoch: 2 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 3 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 3 [100/900 (11%)]\tLoss: 0.713262\n",
      "Train Epoch: 3 [200/900 (22%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 3 [400/900 (44%)]\tLoss: 0.513262\n",
      "Train Epoch: 3 [500/900 (56%)]\tLoss: 0.613262\n",
      "Train Epoch: 3 [600/900 (67%)]\tLoss: 1.013262\n",
      "Train Epoch: 3 [700/900 (78%)]\tLoss: 0.713262\n",
      "Train Epoch: 3 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 4 [0/900 (0%)]\tLoss: 0.713262\n",
      "Train Epoch: 4 [100/900 (11%)]\tLoss: 0.513262\n",
      "Train Epoch: 4 [200/900 (22%)]\tLoss: 0.813262\n",
      "Train Epoch: 4 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 4 [400/900 (44%)]\tLoss: 0.813262\n",
      "Train Epoch: 4 [500/900 (56%)]\tLoss: 1.013262\n",
      "Train Epoch: 4 [600/900 (67%)]\tLoss: 0.713262\n",
      "Train Epoch: 4 [700/900 (78%)]\tLoss: 0.713262\n",
      "Train Epoch: 4 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182229, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 5 [0/900 (0%)]\tLoss: 0.613262\n",
      "Train Epoch: 5 [100/900 (11%)]\tLoss: 0.513262\n",
      "Train Epoch: 5 [200/900 (22%)]\tLoss: 0.613262\n",
      "Train Epoch: 5 [300/900 (33%)]\tLoss: 0.913262\n",
      "Train Epoch: 5 [400/900 (44%)]\tLoss: 0.813262\n",
      "Train Epoch: 5 [500/900 (56%)]\tLoss: 0.813262\n",
      "Train Epoch: 5 [600/900 (67%)]\tLoss: 0.513262\n",
      "Train Epoch: 5 [700/900 (78%)]\tLoss: 0.713262\n",
      "Train Epoch: 5 [800/900 (89%)]\tLoss: 0.813262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 6 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 6 [100/900 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 6 [200/900 (22%)]\tLoss: 0.813262\n",
      "Train Epoch: 6 [300/900 (33%)]\tLoss: 0.913262\n",
      "Train Epoch: 6 [400/900 (44%)]\tLoss: 0.813262\n",
      "Train Epoch: 6 [500/900 (56%)]\tLoss: 0.613262\n",
      "Train Epoch: 6 [600/900 (67%)]\tLoss: 1.013262\n",
      "Train Epoch: 6 [700/900 (78%)]\tLoss: 0.713262\n",
      "Train Epoch: 6 [800/900 (89%)]\tLoss: 0.513262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 7 [0/900 (0%)]\tLoss: 0.613262\n",
      "Train Epoch: 7 [100/900 (11%)]\tLoss: 0.913262\n",
      "Train Epoch: 7 [200/900 (22%)]\tLoss: 1.113262\n",
      "Train Epoch: 7 [300/900 (33%)]\tLoss: 1.013262\n",
      "Train Epoch: 7 [400/900 (44%)]\tLoss: 0.713262\n",
      "Train Epoch: 7 [500/900 (56%)]\tLoss: 0.713262\n",
      "Train Epoch: 7 [600/900 (67%)]\tLoss: 0.813262\n",
      "Train Epoch: 7 [700/900 (78%)]\tLoss: 0.713262\n",
      "Train Epoch: 7 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 8 [0/900 (0%)]\tLoss: 0.613262\n",
      "Train Epoch: 8 [100/900 (11%)]\tLoss: 0.413262\n",
      "Train Epoch: 8 [200/900 (22%)]\tLoss: 0.413262\n",
      "Train Epoch: 8 [300/900 (33%)]\tLoss: 0.913262\n",
      "Train Epoch: 8 [400/900 (44%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [500/900 (56%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [600/900 (67%)]\tLoss: 0.813262\n",
      "Train Epoch: 8 [700/900 (78%)]\tLoss: 0.913262\n",
      "Train Epoch: 8 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Train Epoch: 9 [0/900 (0%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [100/900 (11%)]\tLoss: 0.813262\n",
      "Train Epoch: 9 [200/900 (22%)]\tLoss: 0.813262\n",
      "Train Epoch: 9 [300/900 (33%)]\tLoss: 0.813262\n",
      "Train Epoch: 9 [400/900 (44%)]\tLoss: 0.613262\n",
      "Train Epoch: 9 [500/900 (56%)]\tLoss: 0.713262\n",
      "Train Epoch: 9 [600/900 (67%)]\tLoss: 0.613262\n",
      "Train Epoch: 9 [700/900 (78%)]\tLoss: 0.913262\n",
      "Train Epoch: 9 [800/900 (89%)]\tLoss: 0.713262\n",
      "\n",
      "Test set: Average loss: 0.08132616875182228, Accuracy: 50.0%\n",
      "\n",
      "Training sample size = 9000\n",
      "Using all features\n",
      "Testing sample size = 500\n",
      "Using all features\n",
      "Validation sample size = 500\n",
      "Using all features\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.000010.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.695637\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.718025\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.707634\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.723771\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.744008\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.658974\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.717451\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.715075\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.726751\n",
      "\n",
      "Test set: Average loss: 0.007102640452349401, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.720706\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.685355\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.700661\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.718207\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.722250\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.717035\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.740200\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.725806\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.718948\n",
      "\n",
      "Test set: Average loss: 0.007099021711874678, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.704730\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.714310\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.718097\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.719809\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.722749\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.712064\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.701337\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.690944\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.697514\n",
      "\n",
      "Test set: Average loss: 0.0070954489505699134, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.708105\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.706803\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.686212\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.679900\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.726382\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.738581\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.699624\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.719084\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.736477\n",
      "\n",
      "Test set: Average loss: 0.007091926842986507, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.727857\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.711256\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.725282\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.719544\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.709658\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.726890\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.700211\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.713764\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.735858\n",
      "\n",
      "Test set: Average loss: 0.007088403825638246, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.720430\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.691681\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.690440\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.739761\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.736755\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.713920\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.734989\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.689080\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.705343\n",
      "\n",
      "Test set: Average loss: 0.00708507452961327, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.728528\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.684418\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.722706\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.703980\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.709734\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.704544\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.715766\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.706220\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.741863\n",
      "\n",
      "Test set: Average loss: 0.007081680466457163, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.707569\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.697114\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.713533\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.696146\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.725580\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.714976\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.721033\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.698567\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.707950\n",
      "\n",
      "Test set: Average loss: 0.007078413423285867, Accuracy: 47.8%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.714274\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.700446\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.698460\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.742548\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.703885\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.714818\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.685538\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.723086\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.723379\n",
      "\n",
      "Test set: Average loss: 0.0070751387913180926, Accuracy: 48.0%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.712271\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.753132\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.704396\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.705654\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.710755\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.697312\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.729158\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.692184\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.730238\n",
      "\n",
      "Test set: Average loss: 0.0070720358387416065, Accuracy: 48.0%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.000100.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.688509\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.687883\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.691178\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.701775\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.691032\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.695555\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.700465\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.695447\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.699551\n",
      "\n",
      "Test set: Average loss: 0.006951629903019214, Accuracy: 51.0%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.685249\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.693860\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.695275\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.682726\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.697958\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.703916\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.714769\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.709266\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.700009\n",
      "\n",
      "Test set: Average loss: 0.006950922382118037, Accuracy: 51.0%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.700517\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.693026\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.700885\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.697868\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.677428\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.706721\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.687814\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.716925\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.705292\n",
      "\n",
      "Test set: Average loss: 0.006950278981951067, Accuracy: 51.4%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.681715\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.701603\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.678531\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.698167\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.695160\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.687572\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.695875\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.710417\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.671247\n",
      "\n",
      "Test set: Average loss: 0.006949575402496224, Accuracy: 51.4%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.696962\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.699105\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.718522\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.689687\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.692245\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.692546\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.695382\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.699024\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.691658\n",
      "\n",
      "Test set: Average loss: 0.0069489735995473, Accuracy: 51.800000000000004%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.697118\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.692608\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.698338\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.688679\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.718759\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.694940\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.715483\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.699324\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.699470\n",
      "\n",
      "Test set: Average loss: 0.006948391630120894, Accuracy: 52.0%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.717764\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.699881\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.691377\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.696174\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.711772\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.689172\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.707822\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.692575\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.722560\n",
      "\n",
      "Test set: Average loss: 0.0069477614718570935, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.703578\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.715916\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.686851\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.710585\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.683992\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.699578\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.695363\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.688895\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.701600\n",
      "\n",
      "Test set: Average loss: 0.006947102679774007, Accuracy: 52.400000000000006%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.698821\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.694871\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.687416\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.685502\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.683869\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.695243\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.695086\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.684284\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.705889\n",
      "\n",
      "Test set: Average loss: 0.006946562812620734, Accuracy: 51.800000000000004%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.706956\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.698120\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.686619\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.692744\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.666599\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.713322\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.702375\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.696761\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.698331\n",
      "\n",
      "Test set: Average loss: 0.006945907763316393, Accuracy: 51.4%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.001000.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.711774\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.695307\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.689143\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.714592\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.693652\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.685178\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.686776\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.693700\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.699351\n",
      "\n",
      "Test set: Average loss: 0.0069314526433339055, Accuracy: 50.4%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.688954\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.701149\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.714299\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.680263\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.685187\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.710353\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.704427\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.709145\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.682679\n",
      "\n",
      "Test set: Average loss: 0.006923811982344719, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.678882\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.690840\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.714614\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.706869\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.710778\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.683006\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.685865\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.713939\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.702336\n",
      "\n",
      "Test set: Average loss: 0.006916026779706635, Accuracy: 52.400000000000006%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.708761\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.694306\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.685465\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.712670\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.690359\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.707503\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.704659\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.710591\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.687409\n",
      "\n",
      "Test set: Average loss: 0.0069092295547727, Accuracy: 53.800000000000004%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.694248\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.693024\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.692079\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.683926\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.702999\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.694761\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.689773\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.680943\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.684019\n",
      "\n",
      "Test set: Average loss: 0.006904041096416652, Accuracy: 53.6%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.694096\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.687856\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.696625\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.688030\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.713331\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.677162\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.689917\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.698616\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.696349\n",
      "\n",
      "Test set: Average loss: 0.006898278297101132, Accuracy: 53.6%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.687716\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.690423\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.702352\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.697556\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.682749\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.667609\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.689828\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.710147\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.704783\n",
      "\n",
      "Test set: Average loss: 0.006894418933139378, Accuracy: 54.0%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.711381\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.707210\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.685840\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.694694\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.705202\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.702699\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.691135\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.693235\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.681622\n",
      "\n",
      "Test set: Average loss: 0.006889031763090465, Accuracy: 53.6%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.678255\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.689283\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.675383\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.694268\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.680928\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.689884\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.704335\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.697970\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.687458\n",
      "\n",
      "Test set: Average loss: 0.0068843566827708645, Accuracy: 53.2%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.687842\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.676287\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.672436\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.682711\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.704081\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.686252\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.685450\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.701247\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.697340\n",
      "\n",
      "Test set: Average loss: 0.006877606695933192, Accuracy: 54.2%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.010000.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.676843\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.714018\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.696375\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.700385\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.681279\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.703694\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.674948\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.691084\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.689505\n",
      "\n",
      "Test set: Average loss: 0.006887620264082913, Accuracy: 52.6%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.699984\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.688555\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.682141\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.693432\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.709100\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.670173\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.687223\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.683693\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.669786\n",
      "\n",
      "Test set: Average loss: 0.006834579414591704, Accuracy: 55.800000000000004%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.669181\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.700689\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.685885\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.673419\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.670650\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.672567\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.673288\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.666585\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.689309\n",
      "\n",
      "Test set: Average loss: 0.006803172449677392, Accuracy: 57.199999999999996%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.662870\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.684659\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.674410\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.691389\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.664887\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.669869\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.675355\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.679559\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.687178\n",
      "\n",
      "Test set: Average loss: 0.0067698683352788456, Accuracy: 59.8%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.687392\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.673807\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.693467\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.688721\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.684972\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.683130\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.683862\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.682852\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.673581\n",
      "\n",
      "Test set: Average loss: 0.006757271134345691, Accuracy: 59.0%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.673235\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.674158\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.674328\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.674559\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.670965\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.665721\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.697869\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.673904\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.695360\n",
      "\n",
      "Test set: Average loss: 0.006731618453410558, Accuracy: 57.8%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.699971\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.658498\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.692727\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.673674\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.637968\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.698516\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.665418\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.670140\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.655529\n",
      "\n",
      "Test set: Average loss: 0.006718395382705646, Accuracy: 58.4%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.674293\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.661198\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.660873\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.682231\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.700992\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.676083\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.678435\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.666957\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.675381\n",
      "\n",
      "Test set: Average loss: 0.006720698551068441, Accuracy: 58.8%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.676303\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.659031\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.680749\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.691024\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.665370\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.660102\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.657636\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.671696\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.658492\n",
      "\n",
      "Test set: Average loss: 0.006705450805545965, Accuracy: 57.599999999999994%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.666859\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.684530\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.700001\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.666936\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.686380\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.671391\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.653826\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.661166\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.677709\n",
      "\n",
      "Test set: Average loss: 0.006680112228524751, Accuracy: 57.199999999999996%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.100000.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.715939\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.706150\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.682445\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.686732\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.677286\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.698284\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.703208\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.660541\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.679361\n",
      "\n",
      "Test set: Average loss: 0.006630883830434025, Accuracy: 61.6%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.678040\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.663324\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.672686\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.691222\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.635157\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.686138\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.678696\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.671240\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.673755\n",
      "\n",
      "Test set: Average loss: 0.006594613649831193, Accuracy: 60.6%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.681194\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.640418\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.636120\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.668130\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.645361\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.698671\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.647404\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.688563\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.629793\n",
      "\n",
      "Test set: Average loss: 0.0065328936328288385, Accuracy: 61.0%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.681684\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.625541\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.708475\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.714164\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.683953\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.689320\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.672882\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.626277\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.650491\n",
      "\n",
      "Test set: Average loss: 0.006519795527585857, Accuracy: 60.6%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.670422\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.618560\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.688924\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.650449\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.644571\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.668973\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.680502\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.627756\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.665247\n",
      "\n",
      "Test set: Average loss: 0.006470238997130015, Accuracy: 64.4%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.619961\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.680951\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.700846\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.599767\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.638129\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.677872\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.621454\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.640756\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.644680\n",
      "\n",
      "Test set: Average loss: 0.006462801274029857, Accuracy: 61.8%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.624182\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.665225\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.634911\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.618319\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.665100\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.613665\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.679460\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.626296\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.625764\n",
      "\n",
      "Test set: Average loss: 0.006465791234241683, Accuracy: 64.8%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.651349\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.638057\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.653243\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.643653\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.661678\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.617306\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.617523\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.650599\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.638250\n",
      "\n",
      "Test set: Average loss: 0.006445554617031182, Accuracy: 63.6%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.645071\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.657757\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.641094\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.628149\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.643949\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.648649\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.618560\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.631487\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.624746\n",
      "\n",
      "Test set: Average loss: 0.0063832454620326275, Accuracy: 64.4%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.661796\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.658439\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.640248\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.609339\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.617060\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.643790\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.682116\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.621933\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.630640\n",
      "\n",
      "Test set: Average loss: 0.006437726568124541, Accuracy: 61.8%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.200000.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.699982\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.692252\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.675120\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.654275\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.671834\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.688094\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.667676\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.681733\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.671965\n",
      "\n",
      "Test set: Average loss: 0.006715315791329526, Accuracy: 57.99999999999999%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.677094\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.648666\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.645300\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.647602\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.662955\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.666942\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.713644\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.636438\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.660457\n",
      "\n",
      "Test set: Average loss: 0.0065438954483301535, Accuracy: 61.4%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.678401\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.663130\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.677930\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.625699\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.647277\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.678164\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.622872\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.633079\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.697439\n",
      "\n",
      "Test set: Average loss: 0.006527002476022983, Accuracy: 61.4%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.710504\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.626333\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.649139\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.645707\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.668298\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.618668\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.633985\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.642789\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.604506\n",
      "\n",
      "Test set: Average loss: 0.00652748047813588, Accuracy: 60.0%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.710034\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.634120\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.641224\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.669278\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.663126\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.614006\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.648110\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.666374\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.623088\n",
      "\n",
      "Test set: Average loss: 0.006500514479325475, Accuracy: 62.8%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.657716\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.617366\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.604601\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.654062\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.649929\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.642594\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.610314\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.598555\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.672677\n",
      "\n",
      "Test set: Average loss: 0.006432924592890282, Accuracy: 61.6%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.627753\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.635072\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.638607\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.661415\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.653156\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.648896\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.588175\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.654848\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.690179\n",
      "\n",
      "Test set: Average loss: 0.0065808909037055165, Accuracy: 58.199999999999996%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.632000\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.631055\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.672315\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.633259\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.629943\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.574942\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.630608\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.640697\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.640011\n",
      "\n",
      "Test set: Average loss: 0.0064411861159354225, Accuracy: 63.2%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.583649\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.622129\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.652867\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.633914\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.695035\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.646729\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.628055\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.682232\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.652492\n",
      "\n",
      "Test set: Average loss: 0.006580324238633793, Accuracy: 60.0%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.646280\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.647301\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.659778\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.647979\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.599135\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.627419\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.639625\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.628177\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.625117\n",
      "\n",
      "Test set: Average loss: 0.00647378049059061, Accuracy: 61.8%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=0.500000.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.690566\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.704918\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.718351\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.731853\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.657383\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.761248\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.682059\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.689971\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.704592\n",
      "\n",
      "Test set: Average loss: 0.007227032013398918, Accuracy: 53.0%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.762207\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.690060\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.647842\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.675638\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.658267\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.686832\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.685714\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.637020\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.659062\n",
      "\n",
      "Test set: Average loss: 0.0067514672656149135, Accuracy: 57.4%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.665385\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.631333\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.665400\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.672817\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.671869\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.669359\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.664961\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.686818\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.651417\n",
      "\n",
      "Test set: Average loss: 0.006645496032543632, Accuracy: 60.8%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.649540\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.684629\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.656019\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.672545\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.680085\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.648354\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.701325\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.622645\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.704447\n",
      "\n",
      "Test set: Average loss: 0.006489807533879361, Accuracy: 63.2%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.674903\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.645065\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.646642\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.642413\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.661691\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.654852\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.660398\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.696979\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.700961\n",
      "\n",
      "Test set: Average loss: 0.006496251429159363, Accuracy: 60.6%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.704693\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.711218\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.638303\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.587604\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.647541\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.685515\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.639516\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.631588\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.635620\n",
      "\n",
      "Test set: Average loss: 0.006490442846545672, Accuracy: 62.4%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.656432\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.663829\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.596392\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.680960\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.639183\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.636215\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.676217\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.641542\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.636105\n",
      "\n",
      "Test set: Average loss: 0.006526576246981175, Accuracy: 61.199999999999996%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.634280\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.644014\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.643958\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.591839\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.637153\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.711286\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.668313\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.656877\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.744067\n",
      "\n",
      "Test set: Average loss: 0.006501469841576877, Accuracy: 62.2%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.609161\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.680308\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.620382\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.622903\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.656674\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.657297\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.646173\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.641896\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.668506\n",
      "\n",
      "Test set: Average loss: 0.006457548496695972, Accuracy: 63.2%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.666361\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.715503\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.586411\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.689055\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.669218\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.643992\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.636903\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.625746\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.655785\n",
      "\n",
      "Test set: Average loss: 0.006587677449035942, Accuracy: 60.0%\n",
      "\n",
      "\n",
      "Training DNN with 10000 data points and SGD lr=1.000000.\n",
      "\n",
      "Train Epoch: 0 [0/9000 (0%)]\tLoss: 0.721093\n",
      "Train Epoch: 0 [1000/9000 (11%)]\tLoss: 0.700719\n",
      "Train Epoch: 0 [2000/9000 (22%)]\tLoss: 0.776755\n",
      "Train Epoch: 0 [3000/9000 (33%)]\tLoss: 0.764027\n",
      "Train Epoch: 0 [4000/9000 (44%)]\tLoss: 0.740971\n",
      "Train Epoch: 0 [5000/9000 (56%)]\tLoss: 0.830172\n",
      "Train Epoch: 0 [6000/9000 (67%)]\tLoss: 0.713043\n",
      "Train Epoch: 0 [7000/9000 (78%)]\tLoss: 0.830348\n",
      "Train Epoch: 0 [8000/9000 (89%)]\tLoss: 0.865812\n",
      "\n",
      "Test set: Average loss: 0.007508201725203078, Accuracy: 54.400000000000006%\n",
      "\n",
      "Train Epoch: 1 [0/9000 (0%)]\tLoss: 0.891567\n",
      "Train Epoch: 1 [1000/9000 (11%)]\tLoss: 0.869983\n",
      "Train Epoch: 1 [2000/9000 (22%)]\tLoss: 0.822561\n",
      "Train Epoch: 1 [3000/9000 (33%)]\tLoss: 0.758457\n",
      "Train Epoch: 1 [4000/9000 (44%)]\tLoss: 0.823513\n",
      "Train Epoch: 1 [5000/9000 (56%)]\tLoss: 0.783242\n",
      "Train Epoch: 1 [6000/9000 (67%)]\tLoss: 0.803259\n",
      "Train Epoch: 1 [7000/9000 (78%)]\tLoss: 0.823269\n",
      "Train Epoch: 1 [8000/9000 (89%)]\tLoss: 0.773260\n",
      "\n",
      "Test set: Average loss: 0.007912616875182228, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 2 [0/9000 (0%)]\tLoss: 0.794905\n",
      "Train Epoch: 2 [1000/9000 (11%)]\tLoss: 0.715417\n",
      "Train Epoch: 2 [2000/9000 (22%)]\tLoss: 0.823266\n",
      "Train Epoch: 2 [3000/9000 (33%)]\tLoss: 0.803269\n",
      "Train Epoch: 2 [4000/9000 (44%)]\tLoss: 0.723208\n",
      "Train Epoch: 2 [5000/9000 (56%)]\tLoss: 0.773139\n",
      "Train Epoch: 2 [6000/9000 (67%)]\tLoss: 0.812994\n",
      "Train Epoch: 2 [7000/9000 (78%)]\tLoss: 0.853758\n",
      "Train Epoch: 2 [8000/9000 (89%)]\tLoss: 0.774307\n",
      "\n",
      "Test set: Average loss: 0.007912616875181432, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 3 [0/9000 (0%)]\tLoss: 0.737449\n",
      "Train Epoch: 3 [1000/9000 (11%)]\tLoss: 0.871969\n",
      "Train Epoch: 3 [2000/9000 (22%)]\tLoss: 0.708582\n",
      "Train Epoch: 3 [3000/9000 (33%)]\tLoss: 0.859254\n",
      "Train Epoch: 3 [4000/9000 (44%)]\tLoss: 0.819774\n",
      "Train Epoch: 3 [5000/9000 (56%)]\tLoss: 0.750103\n",
      "Train Epoch: 3 [6000/9000 (67%)]\tLoss: 0.749523\n",
      "Train Epoch: 3 [7000/9000 (78%)]\tLoss: 0.804889\n",
      "Train Epoch: 3 [8000/9000 (89%)]\tLoss: 0.866020\n",
      "\n",
      "Test set: Average loss: 0.007912616875182226, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 4 [0/9000 (0%)]\tLoss: 0.754689\n",
      "Train Epoch: 4 [1000/9000 (11%)]\tLoss: 0.742834\n",
      "Train Epoch: 4 [2000/9000 (22%)]\tLoss: 0.714229\n",
      "Train Epoch: 4 [3000/9000 (33%)]\tLoss: 0.831927\n",
      "Train Epoch: 4 [4000/9000 (44%)]\tLoss: 0.787200\n",
      "Train Epoch: 4 [5000/9000 (56%)]\tLoss: 0.846099\n",
      "Train Epoch: 4 [6000/9000 (67%)]\tLoss: 0.744618\n",
      "Train Epoch: 4 [7000/9000 (78%)]\tLoss: 0.777351\n",
      "Train Epoch: 4 [8000/9000 (89%)]\tLoss: 0.759161\n",
      "\n",
      "Test set: Average loss: 0.007912616875182228, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 5 [0/9000 (0%)]\tLoss: 0.790546\n",
      "Train Epoch: 5 [1000/9000 (11%)]\tLoss: 0.820155\n",
      "Train Epoch: 5 [2000/9000 (22%)]\tLoss: 0.800717\n",
      "Train Epoch: 5 [3000/9000 (33%)]\tLoss: 0.708333\n",
      "Train Epoch: 5 [4000/9000 (44%)]\tLoss: 0.716007\n",
      "Train Epoch: 5 [5000/9000 (56%)]\tLoss: 0.732361\n",
      "Train Epoch: 5 [6000/9000 (67%)]\tLoss: 0.758435\n",
      "Train Epoch: 5 [7000/9000 (78%)]\tLoss: 0.738159\n",
      "Train Epoch: 5 [8000/9000 (89%)]\tLoss: 0.747872\n",
      "\n",
      "Test set: Average loss: 0.0079126168751798, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 6 [0/9000 (0%)]\tLoss: 0.793822\n",
      "Train Epoch: 6 [1000/9000 (11%)]\tLoss: 0.837677\n",
      "Train Epoch: 6 [2000/9000 (22%)]\tLoss: 0.739606\n",
      "Train Epoch: 6 [3000/9000 (33%)]\tLoss: 0.756167\n",
      "Train Epoch: 6 [4000/9000 (44%)]\tLoss: 0.733002\n",
      "Train Epoch: 6 [5000/9000 (56%)]\tLoss: 0.818822\n",
      "Train Epoch: 6 [6000/9000 (67%)]\tLoss: 0.802701\n",
      "Train Epoch: 6 [7000/9000 (78%)]\tLoss: 0.860214\n",
      "Train Epoch: 6 [8000/9000 (89%)]\tLoss: 0.803434\n",
      "\n",
      "Test set: Average loss: 0.007912616875182228, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 7 [0/9000 (0%)]\tLoss: 0.733439\n",
      "Train Epoch: 7 [1000/9000 (11%)]\tLoss: 0.713531\n",
      "Train Epoch: 7 [2000/9000 (22%)]\tLoss: 0.869797\n",
      "Train Epoch: 7 [3000/9000 (33%)]\tLoss: 0.789393\n",
      "Train Epoch: 7 [4000/9000 (44%)]\tLoss: 0.721802\n",
      "Train Epoch: 7 [5000/9000 (56%)]\tLoss: 0.813280\n",
      "Train Epoch: 7 [6000/9000 (67%)]\tLoss: 0.802097\n",
      "Train Epoch: 7 [7000/9000 (78%)]\tLoss: 0.872070\n",
      "Train Epoch: 7 [8000/9000 (89%)]\tLoss: 0.794915\n",
      "\n",
      "Test set: Average loss: 0.007912616875182228, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 8 [0/9000 (0%)]\tLoss: 0.764978\n",
      "Train Epoch: 8 [1000/9000 (11%)]\tLoss: 0.761815\n",
      "Train Epoch: 8 [2000/9000 (22%)]\tLoss: 0.755403\n",
      "Train Epoch: 8 [3000/9000 (33%)]\tLoss: 0.748081\n",
      "Train Epoch: 8 [4000/9000 (44%)]\tLoss: 0.775245\n",
      "Train Epoch: 8 [5000/9000 (56%)]\tLoss: 0.700018\n",
      "Train Epoch: 8 [6000/9000 (67%)]\tLoss: 0.788782\n",
      "Train Epoch: 8 [7000/9000 (78%)]\tLoss: 0.796079\n",
      "Train Epoch: 8 [8000/9000 (89%)]\tLoss: 0.774300\n",
      "\n",
      "Test set: Average loss: 0.007912616875182226, Accuracy: 52.2%\n",
      "\n",
      "Train Epoch: 9 [0/9000 (0%)]\tLoss: 0.823206\n",
      "Train Epoch: 9 [1000/9000 (11%)]\tLoss: 0.764656\n",
      "Train Epoch: 9 [2000/9000 (22%)]\tLoss: 0.679817\n",
      "Train Epoch: 9 [3000/9000 (33%)]\tLoss: 0.687626\n",
      "Train Epoch: 9 [4000/9000 (44%)]\tLoss: 0.897869\n",
      "Train Epoch: 9 [5000/9000 (56%)]\tLoss: 0.749289\n",
      "Train Epoch: 9 [6000/9000 (67%)]\tLoss: 0.849507\n",
      "Train Epoch: 9 [7000/9000 (78%)]\tLoss: 0.875109\n",
      "Train Epoch: 9 [8000/9000 (89%)]\tLoss: 0.823502\n",
      "\n",
      "Test set: Average loss: 0.007912616875182228, Accuracy: 52.2%\n",
      "\n",
      "Training sample size = 90000\n",
      "Using all features\n",
      "Testing sample size = 5000\n",
      "Using all features\n",
      "Validation sample size = 5000\n",
      "Using all features\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.000010.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.716125\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.725920\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.722488\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.725333\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.709464\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.722577\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.713771\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.715461\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.716616\n",
      "\n",
      "Test set: Average loss: 0.0007116249135824393, Accuracy: 48.120000000000005%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.720450\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.716007\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.717165\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.706975\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.717698\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.708709\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.724385\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.724365\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.718813\n",
      "\n",
      "Test set: Average loss: 0.0007110869548618491, Accuracy: 48.1%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.708407\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.715684\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.730341\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.714389\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.712085\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.716003\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.725115\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.724556\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.705743\n",
      "\n",
      "Test set: Average loss: 0.0007105609855309167, Accuracy: 48.1%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.711681\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.706046\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.715660\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.721339\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.721502\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.723196\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.717276\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.716753\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.714185\n",
      "\n",
      "Test set: Average loss: 0.0007100504527487253, Accuracy: 48.120000000000005%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.719825\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.721733\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.719566\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.714405\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.709942\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.714596\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.721896\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.722284\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.720886\n",
      "\n",
      "Test set: Average loss: 0.0007095494964730479, Accuracy: 48.04%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.721060\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.713353\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.716188\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.713147\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.715097\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.714261\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.702018\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.714853\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.715542\n",
      "\n",
      "Test set: Average loss: 0.0007090609303366391, Accuracy: 48.02%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.711477\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.722215\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.715906\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.714789\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.727932\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.718319\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.711981\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.721101\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.712023\n",
      "\n",
      "Test set: Average loss: 0.0007085825702908592, Accuracy: 48.04%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.719510\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.709003\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.712709\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.705172\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.712103\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.713686\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.710765\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.714138\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.703781\n",
      "\n",
      "Test set: Average loss: 0.0007081157897194699, Accuracy: 47.96%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.712738\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.704878\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.714488\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.716087\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.696423\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.710522\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.714228\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.715883\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.716448\n",
      "\n",
      "Test set: Average loss: 0.0007076590451734841, Accuracy: 47.980000000000004%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.717584\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.707337\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.707568\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.720668\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.712530\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.727537\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.714683\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.711688\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.710813\n",
      "\n",
      "Test set: Average loss: 0.0007072149765679703, Accuracy: 47.94%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.000100.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.697276\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.699112\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.698735\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.695321\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.695361\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.694159\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.698691\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.696319\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.696455\n",
      "\n",
      "Test set: Average loss: 0.0006957074094936375, Accuracy: 48.66%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.703479\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.694170\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.698546\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.694727\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.690960\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.693196\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.695538\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.696322\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.700337\n",
      "\n",
      "Test set: Average loss: 0.0006956268463519626, Accuracy: 48.94%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.702378\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.695148\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.696199\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.694486\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.695207\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.699175\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.696447\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.696254\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.693501\n",
      "\n",
      "Test set: Average loss: 0.0006955563890874217, Accuracy: 49.26%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.699253\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.697021\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.696932\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.695440\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.694901\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.694305\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.694476\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.701894\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.702331\n",
      "\n",
      "Test set: Average loss: 0.0006954903157133315, Accuracy: 49.1%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.701097\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.699345\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.696453\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.697933\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.693667\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.698478\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.695210\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.696721\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.694479\n",
      "\n",
      "Test set: Average loss: 0.0006954270241424757, Accuracy: 49.26%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.691044\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.702480\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.700190\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.694209\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.696281\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.693772\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.696226\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.701714\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.698525\n",
      "\n",
      "Test set: Average loss: 0.0006953694475376208, Accuracy: 49.74%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.699493\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.698257\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.695195\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.697446\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.696750\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.704414\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.696733\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.693233\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.699860\n",
      "\n",
      "Test set: Average loss: 0.0006953144175087171, Accuracy: 49.96%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.693266\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.692830\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.697642\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.701334\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.696292\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.694451\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.690132\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.695726\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.701024\n",
      "\n",
      "Test set: Average loss: 0.0006952589295947827, Accuracy: 50.06%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.696592\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.701947\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.692789\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.697565\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.692114\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.695921\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.693493\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.695169\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.697249\n",
      "\n",
      "Test set: Average loss: 0.0006951983700016928, Accuracy: 49.94%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.698015\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.697314\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.695509\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.692384\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.698482\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.700225\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.695275\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.692254\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.697763\n",
      "\n",
      "Test set: Average loss: 0.0006951372364504229, Accuracy: 49.980000000000004%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.001000.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.700878\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.697402\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.707724\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.689865\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.700366\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.698894\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.693816\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.700473\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.697898\n",
      "\n",
      "Test set: Average loss: 0.0006928379841479044, Accuracy: 51.459999999999994%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.698449\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.702719\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.697786\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.695701\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.697908\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.694856\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.694342\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.695312\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.696445\n",
      "\n",
      "Test set: Average loss: 0.000692518243628768, Accuracy: 51.800000000000004%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.699387\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.698498\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.694073\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.692742\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.698964\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.695290\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.697890\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.696569\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.694559\n",
      "\n",
      "Test set: Average loss: 0.0006920814582589638, Accuracy: 52.239999999999995%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.689345\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.696338\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.690120\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.697909\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.692071\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.692653\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.696394\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.697418\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.691585\n",
      "\n",
      "Test set: Average loss: 0.0006916772438075275, Accuracy: 52.459999999999994%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.692136\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.693811\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.696756\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.698936\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.691094\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.692612\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.695490\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.695685\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.692885\n",
      "\n",
      "Test set: Average loss: 0.0006912230776046746, Accuracy: 52.980000000000004%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.702647\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.696026\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.694512\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.691164\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.692870\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.692218\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.699055\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.693687\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.694992\n",
      "\n",
      "Test set: Average loss: 0.0006908632732357956, Accuracy: 53.16%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.701201\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.692462\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.694041\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.685118\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.692537\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.691476\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.693388\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.692624\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.685969\n",
      "\n",
      "Test set: Average loss: 0.000690544035647949, Accuracy: 53.239999999999995%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.693267\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.692640\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.693847\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.688008\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.690412\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.689506\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.693572\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.688506\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.690791\n",
      "\n",
      "Test set: Average loss: 0.0006900768763260473, Accuracy: 53.74%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.688704\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.696854\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.688949\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.689812\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.687451\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.689696\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.688012\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.693579\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.688786\n",
      "\n",
      "Test set: Average loss: 0.0006897300544294749, Accuracy: 53.879999999999995%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.692733\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.690409\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.694425\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.694013\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.696562\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.691967\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.688639\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.695437\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.694046\n",
      "\n",
      "Test set: Average loss: 0.00068940862641021, Accuracy: 53.98%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.010000.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.697439\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.703153\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.702923\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.694503\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.696133\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.697888\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.687351\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.690603\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.687412\n",
      "\n",
      "Test set: Average loss: 0.0006892480583556133, Accuracy: 54.120000000000005%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.695987\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.694279\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.688607\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.688630\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.685187\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.688862\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.690310\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.691683\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.688932\n",
      "\n",
      "Test set: Average loss: 0.0006859955521151439, Accuracy: 56.58%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.690164\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.687918\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.686802\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.684417\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.684848\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.687292\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.684667\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.689400\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.685447\n",
      "\n",
      "Test set: Average loss: 0.0006839309453421883, Accuracy: 57.199999999999996%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.685143\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.684645\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.684563\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.682979\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.681018\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.686429\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.685388\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.680476\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.686532\n",
      "\n",
      "Test set: Average loss: 0.0006817835210251876, Accuracy: 57.4%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.684208\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.680904\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.678277\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.678091\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.681826\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.679579\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.678284\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.685269\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.684268\n",
      "\n",
      "Test set: Average loss: 0.0006797654000084675, Accuracy: 58.06%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.682170\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.679645\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.683405\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.676266\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.678329\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.681132\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.682728\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.685005\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.680596\n",
      "\n",
      "Test set: Average loss: 0.0006789029392003156, Accuracy: 57.9%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.680956\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.680117\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.674345\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.679123\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.688512\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.681940\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.671189\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.672999\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.678836\n",
      "\n",
      "Test set: Average loss: 0.0006764325361831883, Accuracy: 59.3%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.681653\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.673717\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.674519\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.679943\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.675955\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.679831\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.684687\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.679343\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.675335\n",
      "\n",
      "Test set: Average loss: 0.0006751226874685315, Accuracy: 59.3%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.670216\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.674042\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.676250\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.689221\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.677774\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.669081\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.674286\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.673649\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.675099\n",
      "\n",
      "Test set: Average loss: 0.0006741038055967786, Accuracy: 59.38%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.679919\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.675449\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.674344\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.667526\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.675990\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.680239\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.680821\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.674472\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.677398\n",
      "\n",
      "Test set: Average loss: 0.0006725752363545446, Accuracy: 59.84%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.100000.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.698679\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.693815\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.693790\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.681532\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.676730\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.690455\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.676593\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.671660\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.677655\n",
      "\n",
      "Test set: Average loss: 0.0006750952803208874, Accuracy: 59.12%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.685068\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.678644\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.673468\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.675094\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.676538\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.685277\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.675249\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.668575\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.663123\n",
      "\n",
      "Test set: Average loss: 0.0006657262151591208, Accuracy: 60.199999999999996%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.665523\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.667240\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.677055\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.659128\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.666360\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.663335\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.670850\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.664604\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.665731\n",
      "\n",
      "Test set: Average loss: 0.0006575057189485098, Accuracy: 61.31999999999999%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.664223\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.662086\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.662097\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.667512\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.663324\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.669284\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.651808\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.661435\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.666184\n",
      "\n",
      "Test set: Average loss: 0.0006524434947013132, Accuracy: 61.660000000000004%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.651450\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.643368\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.653110\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.652355\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.646496\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.667071\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.662848\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.662524\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.668992\n",
      "\n",
      "Test set: Average loss: 0.0006495503195654143, Accuracy: 62.46000000000001%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.654528\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.661750\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.650774\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.646243\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.660604\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.659938\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.640599\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.652032\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.643836\n",
      "\n",
      "Test set: Average loss: 0.0006457590728224826, Accuracy: 63.019999999999996%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.647115\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.642046\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.659248\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.643505\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.643714\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.652507\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.653924\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.658278\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.651907\n",
      "\n",
      "Test set: Average loss: 0.0006433837220886368, Accuracy: 63.06%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.640142\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.633564\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.646532\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.641166\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.634329\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.634558\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.650157\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.642294\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.650014\n",
      "\n",
      "Test set: Average loss: 0.000639121163066968, Accuracy: 63.480000000000004%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.656002\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.633626\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.649996\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.642160\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.637080\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.638873\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.648157\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.627072\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.632893\n",
      "\n",
      "Test set: Average loss: 0.0006379904450744474, Accuracy: 63.36000000000001%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.642050\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.639818\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.626969\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.630970\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.629084\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.628681\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.639704\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.647498\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.639865\n",
      "\n",
      "Test set: Average loss: 0.0006367638612173562, Accuracy: 63.7%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.200000.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.694535\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.690568\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.693110\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.683164\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.680948\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.686204\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.674836\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.670868\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.668385\n",
      "\n",
      "Test set: Average loss: 0.0006654080354932824, Accuracy: 59.760000000000005%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.674994\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.677372\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.660957\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.669100\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.673618\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.668138\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.651665\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.654717\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.655300\n",
      "\n",
      "Test set: Average loss: 0.0006560231793519152, Accuracy: 61.419999999999995%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.660621\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.664001\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.656745\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.641032\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.659710\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.650352\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.660048\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.661373\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.656803\n",
      "\n",
      "Test set: Average loss: 0.0006507225108804939, Accuracy: 62.12%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.645036\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.647792\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.645499\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.629607\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.641408\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.640752\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.660973\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.649599\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.653514\n",
      "\n",
      "Test set: Average loss: 0.000642129983008212, Accuracy: 63.32%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.636233\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.632082\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.632022\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.648940\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.635510\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.652482\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.636169\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.643086\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.652875\n",
      "\n",
      "Test set: Average loss: 0.0006457519841905815, Accuracy: 62.53999999999999%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.655484\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.643489\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.630186\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.628780\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.638908\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.643963\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.626294\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.628063\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.636468\n",
      "\n",
      "Test set: Average loss: 0.0006368637236675723, Accuracy: 63.88%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.626656\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.639597\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.637620\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.644062\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.633441\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.621464\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.620132\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.646475\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.627556\n",
      "\n",
      "Test set: Average loss: 0.0006336459162340542, Accuracy: 64.62%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.633463\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.617769\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.626751\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.640781\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.621097\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.630252\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.620966\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.637606\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.634784\n",
      "\n",
      "Test set: Average loss: 0.0006304552891594299, Accuracy: 65.02%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.637162\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.623758\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.637168\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.642031\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.618703\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.625167\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.653879\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.627810\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.637909\n",
      "\n",
      "Test set: Average loss: 0.000628787869387042, Accuracy: 65.58%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.631176\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.630652\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.619497\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.630505\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.616543\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.612618\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.640131\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.632981\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.613829\n",
      "\n",
      "Test set: Average loss: 0.0006258178900023515, Accuracy: 65.8%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=0.500000.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.702071\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.687777\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.684639\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.678010\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.669341\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.668941\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.666152\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.657582\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.667181\n",
      "\n",
      "Test set: Average loss: 0.0006621411739221542, Accuracy: 60.12%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.653323\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.649534\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.652757\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.641062\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.648719\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.657763\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.640291\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.644856\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.661503\n",
      "\n",
      "Test set: Average loss: 0.0006397212474643664, Accuracy: 63.28%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.643341\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.630760\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.630297\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.636355\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.633433\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.640772\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.644806\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.652594\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.624801\n",
      "\n",
      "Test set: Average loss: 0.0006332741311588315, Accuracy: 64.92%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.633267\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.639975\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.649110\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.635449\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.636907\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.644573\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.629682\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.640533\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.616722\n",
      "\n",
      "Test set: Average loss: 0.0006352482644132487, Accuracy: 64.03999999999999%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.650573\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.612246\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.641341\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.624616\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.632578\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.643238\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.636738\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.636926\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.634349\n",
      "\n",
      "Test set: Average loss: 0.0006283636147488868, Accuracy: 65.53999999999999%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.615682\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.609299\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.631745\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.611218\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.618643\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.605079\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.637790\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.628553\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.636706\n",
      "\n",
      "Test set: Average loss: 0.0006272365641622679, Accuracy: 65.10000000000001%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.622278\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.623424\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.633751\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.631356\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.622435\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.619467\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.624089\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.622573\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.633260\n",
      "\n",
      "Test set: Average loss: 0.000626208362056421, Accuracy: 65.48%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.617487\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.633798\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.634906\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.636132\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.632268\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.618658\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.618358\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.633294\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.621679\n",
      "\n",
      "Test set: Average loss: 0.0006237185624569433, Accuracy: 65.52%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.620404\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.614897\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.624458\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.613185\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.621270\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.630758\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.621822\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.634200\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.610679\n",
      "\n",
      "Test set: Average loss: 0.000621414811581775, Accuracy: 66.08000000000001%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.608702\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.618243\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.620573\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.597167\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.615461\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.602788\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.618478\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.607982\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.644179\n",
      "\n",
      "Test set: Average loss: 0.0006198272052705639, Accuracy: 66.62%\n",
      "\n",
      "\n",
      "Training DNN with 100000 data points and SGD lr=1.000000.\n",
      "\n",
      "Train Epoch: 0 [0/90000 (0%)]\tLoss: 0.715899\n",
      "Train Epoch: 0 [10000/90000 (11%)]\tLoss: 0.684395\n",
      "Train Epoch: 0 [20000/90000 (22%)]\tLoss: 0.673205\n",
      "Train Epoch: 0 [30000/90000 (33%)]\tLoss: 0.689204\n",
      "Train Epoch: 0 [40000/90000 (44%)]\tLoss: 0.677133\n",
      "Train Epoch: 0 [50000/90000 (56%)]\tLoss: 0.666326\n",
      "Train Epoch: 0 [60000/90000 (67%)]\tLoss: 0.665880\n",
      "Train Epoch: 0 [70000/90000 (78%)]\tLoss: 0.664079\n",
      "Train Epoch: 0 [80000/90000 (89%)]\tLoss: 0.664663\n",
      "\n",
      "Test set: Average loss: 0.0006477959470522201, Accuracy: 61.839999999999996%\n",
      "\n",
      "Train Epoch: 1 [0/90000 (0%)]\tLoss: 0.641295\n",
      "Train Epoch: 1 [10000/90000 (11%)]\tLoss: 0.633524\n",
      "Train Epoch: 1 [20000/90000 (22%)]\tLoss: 0.644448\n",
      "Train Epoch: 1 [30000/90000 (33%)]\tLoss: 0.636292\n",
      "Train Epoch: 1 [40000/90000 (44%)]\tLoss: 0.658182\n",
      "Train Epoch: 1 [50000/90000 (56%)]\tLoss: 0.650829\n",
      "Train Epoch: 1 [60000/90000 (67%)]\tLoss: 0.630507\n",
      "Train Epoch: 1 [70000/90000 (78%)]\tLoss: 0.646702\n",
      "Train Epoch: 1 [80000/90000 (89%)]\tLoss: 0.644491\n",
      "\n",
      "Test set: Average loss: 0.0006352294750159598, Accuracy: 64.96%\n",
      "\n",
      "Train Epoch: 2 [0/90000 (0%)]\tLoss: 0.637747\n",
      "Train Epoch: 2 [10000/90000 (11%)]\tLoss: 0.634811\n",
      "Train Epoch: 2 [20000/90000 (22%)]\tLoss: 0.642878\n",
      "Train Epoch: 2 [30000/90000 (33%)]\tLoss: 0.629825\n",
      "Train Epoch: 2 [40000/90000 (44%)]\tLoss: 0.638776\n",
      "Train Epoch: 2 [50000/90000 (56%)]\tLoss: 0.639535\n",
      "Train Epoch: 2 [60000/90000 (67%)]\tLoss: 0.641990\n",
      "Train Epoch: 2 [70000/90000 (78%)]\tLoss: 0.625838\n",
      "Train Epoch: 2 [80000/90000 (89%)]\tLoss: 0.634090\n",
      "\n",
      "Test set: Average loss: 0.0006342058710483673, Accuracy: 64.84%\n",
      "\n",
      "Train Epoch: 3 [0/90000 (0%)]\tLoss: 0.634206\n",
      "Train Epoch: 3 [10000/90000 (11%)]\tLoss: 0.639021\n",
      "Train Epoch: 3 [20000/90000 (22%)]\tLoss: 0.637051\n",
      "Train Epoch: 3 [30000/90000 (33%)]\tLoss: 0.641420\n",
      "Train Epoch: 3 [40000/90000 (44%)]\tLoss: 0.630541\n",
      "Train Epoch: 3 [50000/90000 (56%)]\tLoss: 0.618840\n",
      "Train Epoch: 3 [60000/90000 (67%)]\tLoss: 0.634539\n",
      "Train Epoch: 3 [70000/90000 (78%)]\tLoss: 0.621870\n",
      "Train Epoch: 3 [80000/90000 (89%)]\tLoss: 0.620010\n",
      "\n",
      "Test set: Average loss: 0.0006287108299101265, Accuracy: 65.78%\n",
      "\n",
      "Train Epoch: 4 [0/90000 (0%)]\tLoss: 0.629592\n",
      "Train Epoch: 4 [10000/90000 (11%)]\tLoss: 0.619337\n",
      "Train Epoch: 4 [20000/90000 (22%)]\tLoss: 0.616067\n",
      "Train Epoch: 4 [30000/90000 (33%)]\tLoss: 0.642087\n",
      "Train Epoch: 4 [40000/90000 (44%)]\tLoss: 0.634103\n",
      "Train Epoch: 4 [50000/90000 (56%)]\tLoss: 0.621537\n",
      "Train Epoch: 4 [60000/90000 (67%)]\tLoss: 0.644081\n",
      "Train Epoch: 4 [70000/90000 (78%)]\tLoss: 0.634951\n",
      "Train Epoch: 4 [80000/90000 (89%)]\tLoss: 0.632901\n",
      "\n",
      "Test set: Average loss: 0.000628374862468343, Accuracy: 65.44%\n",
      "\n",
      "Train Epoch: 5 [0/90000 (0%)]\tLoss: 0.625444\n",
      "Train Epoch: 5 [10000/90000 (11%)]\tLoss: 0.634423\n",
      "Train Epoch: 5 [20000/90000 (22%)]\tLoss: 0.626025\n",
      "Train Epoch: 5 [30000/90000 (33%)]\tLoss: 0.609453\n",
      "Train Epoch: 5 [40000/90000 (44%)]\tLoss: 0.625169\n",
      "Train Epoch: 5 [50000/90000 (56%)]\tLoss: 0.630723\n",
      "Train Epoch: 5 [60000/90000 (67%)]\tLoss: 0.618593\n",
      "Train Epoch: 5 [70000/90000 (78%)]\tLoss: 0.616058\n",
      "Train Epoch: 5 [80000/90000 (89%)]\tLoss: 0.631465\n",
      "\n",
      "Test set: Average loss: 0.0006262806781363068, Accuracy: 65.22%\n",
      "\n",
      "Train Epoch: 6 [0/90000 (0%)]\tLoss: 0.635205\n",
      "Train Epoch: 6 [10000/90000 (11%)]\tLoss: 0.619478\n",
      "Train Epoch: 6 [20000/90000 (22%)]\tLoss: 0.638441\n",
      "Train Epoch: 6 [30000/90000 (33%)]\tLoss: 0.627457\n",
      "Train Epoch: 6 [40000/90000 (44%)]\tLoss: 0.662081\n",
      "Train Epoch: 6 [50000/90000 (56%)]\tLoss: 0.628249\n",
      "Train Epoch: 6 [60000/90000 (67%)]\tLoss: 0.613720\n",
      "Train Epoch: 6 [70000/90000 (78%)]\tLoss: 0.605125\n",
      "Train Epoch: 6 [80000/90000 (89%)]\tLoss: 0.620334\n",
      "\n",
      "Test set: Average loss: 0.0006265464482166446, Accuracy: 65.66%\n",
      "\n",
      "Train Epoch: 7 [0/90000 (0%)]\tLoss: 0.618752\n",
      "Train Epoch: 7 [10000/90000 (11%)]\tLoss: 0.637337\n",
      "Train Epoch: 7 [20000/90000 (22%)]\tLoss: 0.645637\n",
      "Train Epoch: 7 [30000/90000 (33%)]\tLoss: 0.640614\n",
      "Train Epoch: 7 [40000/90000 (44%)]\tLoss: 0.612591\n",
      "Train Epoch: 7 [50000/90000 (56%)]\tLoss: 0.634287\n",
      "Train Epoch: 7 [60000/90000 (67%)]\tLoss: 0.622650\n",
      "Train Epoch: 7 [70000/90000 (78%)]\tLoss: 0.611947\n",
      "Train Epoch: 7 [80000/90000 (89%)]\tLoss: 0.615325\n",
      "\n",
      "Test set: Average loss: 0.0006234950394086963, Accuracy: 66.10000000000001%\n",
      "\n",
      "Train Epoch: 8 [0/90000 (0%)]\tLoss: 0.609450\n",
      "Train Epoch: 8 [10000/90000 (11%)]\tLoss: 0.634654\n",
      "Train Epoch: 8 [20000/90000 (22%)]\tLoss: 0.630272\n",
      "Train Epoch: 8 [30000/90000 (33%)]\tLoss: 0.621332\n",
      "Train Epoch: 8 [40000/90000 (44%)]\tLoss: 0.606898\n",
      "Train Epoch: 8 [50000/90000 (56%)]\tLoss: 0.623454\n",
      "Train Epoch: 8 [60000/90000 (67%)]\tLoss: 0.616906\n",
      "Train Epoch: 8 [70000/90000 (78%)]\tLoss: 0.630634\n",
      "Train Epoch: 8 [80000/90000 (89%)]\tLoss: 0.631225\n",
      "\n",
      "Test set: Average loss: 0.0006221730765024326, Accuracy: 66.88%\n",
      "\n",
      "Train Epoch: 9 [0/90000 (0%)]\tLoss: 0.617927\n",
      "Train Epoch: 9 [10000/90000 (11%)]\tLoss: 0.626460\n",
      "Train Epoch: 9 [20000/90000 (22%)]\tLoss: 0.618506\n",
      "Train Epoch: 9 [30000/90000 (33%)]\tLoss: 0.605202\n",
      "Train Epoch: 9 [40000/90000 (44%)]\tLoss: 0.635979\n",
      "Train Epoch: 9 [50000/90000 (56%)]\tLoss: 0.630937\n",
      "Train Epoch: 9 [60000/90000 (67%)]\tLoss: 0.610469\n",
      "Train Epoch: 9 [70000/90000 (78%)]\tLoss: 0.620459\n",
      "Train Epoch: 9 [80000/90000 (89%)]\tLoss: 0.625818\n",
      "\n",
      "Test set: Average loss: 0.0006197182701082626, Accuracy: 66.60000000000001%\n",
      "\n",
      "Training sample size = 180000\n",
      "Using all features\n",
      "Testing sample size = 10000\n",
      "Using all features\n",
      "Validation sample size = 10000\n",
      "Using all features\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.000010.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.700017\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.694563\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.695783\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.697883\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.696714\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.695638\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.694157\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.694178\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.704591\n",
      "\n",
      "Test set: Average loss: 0.0003461345413602111, Accuracy: 53.559999999999995%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.698392\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.692402\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.696254\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.697153\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.695482\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.694454\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.701313\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.696763\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.693811\n",
      "\n",
      "Test set: Average loss: 0.00034612606207651573, Accuracy: 53.559999999999995%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.695039\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.696267\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.690420\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.692727\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.696762\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.696569\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.694328\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.694540\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.695797\n",
      "\n",
      "Test set: Average loss: 0.0003461178397919765, Accuracy: 53.52%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.696128\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.695229\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.697229\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.695893\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.686320\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.692643\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.696549\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.698790\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.696234\n",
      "\n",
      "Test set: Average loss: 0.0003461098900165024, Accuracy: 53.52%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.700508\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.694474\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.698189\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.695990\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.691284\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.698781\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.694743\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.693571\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.697226\n",
      "\n",
      "Test set: Average loss: 0.00034610176145643905, Accuracy: 53.54%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.695366\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.697364\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.701704\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.694571\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.698641\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.699696\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.694351\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.695928\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.695730\n",
      "\n",
      "Test set: Average loss: 0.0003460938363147232, Accuracy: 53.49%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.699659\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.694058\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.696780\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.694585\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.700134\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.691687\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.697829\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.699036\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.698884\n",
      "\n",
      "Test set: Average loss: 0.00034608607559352775, Accuracy: 53.53%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.700205\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.695042\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.698739\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.697724\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.697482\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.697587\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.697484\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.699641\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.691640\n",
      "\n",
      "Test set: Average loss: 0.0003460783978033781, Accuracy: 53.559999999999995%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.696832\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.696612\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.700454\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.693239\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.693208\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.696321\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.699247\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.694014\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.693947\n",
      "\n",
      "Test set: Average loss: 0.00034607089476440983, Accuracy: 53.510000000000005%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.694300\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.696262\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.699114\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.701569\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.699127\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.695379\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.694843\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.691273\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.692418\n",
      "\n",
      "Test set: Average loss: 0.00034606341921367384, Accuracy: 53.53%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.000100.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.698030\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.697311\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.698486\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.699181\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.699924\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.693918\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.695725\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.695576\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.692724\n",
      "\n",
      "Test set: Average loss: 0.0003464537014573234, Accuracy: 53.18000000000001%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.694069\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.698804\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.698031\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.697229\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.697499\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.699342\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.695313\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.699708\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.696027\n",
      "\n",
      "Test set: Average loss: 0.0003463745852309257, Accuracy: 52.949999999999996%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.698866\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.702683\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.700582\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.698091\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.695692\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.698123\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.699152\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.698360\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.697848\n",
      "\n",
      "Test set: Average loss: 0.00034630451570970007, Accuracy: 52.900000000000006%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.696170\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.700757\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.698311\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.701083\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.691506\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.695568\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.702751\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.701688\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.702272\n",
      "\n",
      "Test set: Average loss: 0.0003462443375760593, Accuracy: 52.96999999999999%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.703170\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.700414\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.692202\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.697680\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.693540\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.695674\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.693313\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.693797\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.701177\n",
      "\n",
      "Test set: Average loss: 0.0003461907923573736, Accuracy: 52.88%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.696611\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.697850\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.697167\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.695322\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.699080\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.694811\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.698666\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.695086\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.692102\n",
      "\n",
      "Test set: Average loss: 0.00034614128404697475, Accuracy: 52.93%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.693688\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.694731\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.699124\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.699404\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.695409\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.697479\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.694357\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.698042\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.701674\n",
      "\n",
      "Test set: Average loss: 0.00034609459112317794, Accuracy: 52.910000000000004%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.694831\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.701871\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.698194\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.700672\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.699866\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.693960\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.699216\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.701692\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.696207\n",
      "\n",
      "Test set: Average loss: 0.000346051306394325, Accuracy: 52.86%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.695418\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.689923\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.695743\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.700951\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.693393\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.697560\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.693760\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.694266\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.693077\n",
      "\n",
      "Test set: Average loss: 0.00034601088017064405, Accuracy: 52.910000000000004%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.698760\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.691126\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.696316\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.696530\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.699810\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.695024\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.697796\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.693840\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.698762\n",
      "\n",
      "Test set: Average loss: 0.00034597175078683937, Accuracy: 52.900000000000006%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.001000.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.703311\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.693543\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.697841\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.699454\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.698912\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.698139\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.694368\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.696348\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.697297\n",
      "\n",
      "Test set: Average loss: 0.0003467697193978662, Accuracy: 52.23%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.698962\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.696402\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.700533\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.694491\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.695134\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.698907\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.694864\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.693995\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.697267\n",
      "\n",
      "Test set: Average loss: 0.00034634225678683965, Accuracy: 52.739999999999995%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.693131\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.694497\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.697869\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.696452\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.697245\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.696471\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.693742\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.697618\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.694914\n",
      "\n",
      "Test set: Average loss: 0.0003459550039310895, Accuracy: 52.910000000000004%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.697201\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.696906\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.690151\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.694092\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.694681\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.694142\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.698481\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.696372\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.691521\n",
      "\n",
      "Test set: Average loss: 0.00034559300276549367, Accuracy: 53.23%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.697987\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.696369\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.696928\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.699289\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.693047\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.694874\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.698246\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.693777\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.691614\n",
      "\n",
      "Test set: Average loss: 0.00034525762818246564, Accuracy: 53.49%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.697785\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.693640\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.694289\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.696380\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.691781\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.695984\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.695492\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.689404\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.692075\n",
      "\n",
      "Test set: Average loss: 0.0003449492237096086, Accuracy: 53.72%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.690370\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.692066\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.692551\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.692324\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.690669\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.692026\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.692722\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.687303\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.690835\n",
      "\n",
      "Test set: Average loss: 0.0003446627022163699, Accuracy: 53.93%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.693950\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.693720\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.692556\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.691408\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.691277\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.689770\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.691410\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.694157\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.691595\n",
      "\n",
      "Test set: Average loss: 0.0003443992190971983, Accuracy: 54.1%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.689284\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.690337\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.692094\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.692719\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.690776\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.692617\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.692227\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.688922\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.694641\n",
      "\n",
      "Test set: Average loss: 0.0003441568282798075, Accuracy: 54.21%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.690127\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.689365\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.691072\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.691278\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.689956\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.688619\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.692449\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.692021\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.691942\n",
      "\n",
      "Test set: Average loss: 0.000343915844759303, Accuracy: 54.44%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.010000.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.696181\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.696556\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.698919\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.692634\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.693165\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.692073\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.693691\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.693310\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.687320\n",
      "\n",
      "Test set: Average loss: 0.0003437184060189412, Accuracy: 55.08%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.688992\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.687276\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.686427\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.684668\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.689762\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.688114\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.686831\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.690140\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.685425\n",
      "\n",
      "Test set: Average loss: 0.0003419368774638945, Accuracy: 56.84%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.684243\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.689423\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.689180\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.684935\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.684757\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.689116\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.680704\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.685603\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.688737\n",
      "\n",
      "Test set: Average loss: 0.0003405063029324473, Accuracy: 58.099999999999994%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.682799\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.686921\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.684669\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.684944\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.682214\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.684470\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.684400\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.679671\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.683377\n",
      "\n",
      "Test set: Average loss: 0.00033938335490843924, Accuracy: 58.84%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.685581\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.679754\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.681151\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.680187\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.686462\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.680837\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.678079\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.679924\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.680032\n",
      "\n",
      "Test set: Average loss: 0.00033843985611995526, Accuracy: 58.919999999999995%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.683247\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.682428\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.683073\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.679187\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.681151\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.680498\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.675109\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.680252\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.679618\n",
      "\n",
      "Test set: Average loss: 0.00033752653315656473, Accuracy: 59.58%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.678559\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.678610\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.682326\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.678245\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.673702\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.679758\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.675088\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.679559\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.681864\n",
      "\n",
      "Test set: Average loss: 0.0003366837630460873, Accuracy: 59.9%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.683496\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.673537\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.678591\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.678418\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.677233\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.675707\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.671324\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.677183\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.679922\n",
      "\n",
      "Test set: Average loss: 0.000335906732679595, Accuracy: 60.24%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.671812\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.679891\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.682851\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.679035\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.678836\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.670968\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.674854\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.676697\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.675682\n",
      "\n",
      "Test set: Average loss: 0.000335239102576639, Accuracy: 60.68%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.677738\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.671852\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.673982\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.676428\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.674925\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.670833\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.669771\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.677828\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.676951\n",
      "\n",
      "Test set: Average loss: 0.000334648436319159, Accuracy: 60.67%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.100000.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.699095\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.692181\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.684261\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.686167\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.679343\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.680918\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.674873\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.672173\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.681769\n",
      "\n",
      "Test set: Average loss: 0.0003349879818227957, Accuracy: 60.419999999999995%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.678061\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.673929\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.675939\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.666893\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.676906\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.674978\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.675913\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.669975\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.666405\n",
      "\n",
      "Test set: Average loss: 0.00033005416256975153, Accuracy: 61.31999999999999%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.667584\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.664256\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.662677\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.667214\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.663599\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.662459\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.662373\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.653550\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.660757\n",
      "\n",
      "Test set: Average loss: 0.00032650640386436747, Accuracy: 62.29%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.661580\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.664926\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.653750\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.655152\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.661686\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.655639\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.652375\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.661589\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.652506\n",
      "\n",
      "Test set: Average loss: 0.00032404526246597297, Accuracy: 62.82%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.657370\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.653412\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.657885\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.653711\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.654870\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.657794\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.652670\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.659195\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.656272\n",
      "\n",
      "Test set: Average loss: 0.00032138623450981673, Accuracy: 63.82%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.643795\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.650048\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.663538\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.659809\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.648159\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.648386\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.647419\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.652355\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.650545\n",
      "\n",
      "Test set: Average loss: 0.0003194992689616379, Accuracy: 64.29%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.636162\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.649818\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.642264\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.654722\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.643903\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.642151\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.629747\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.644923\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.639262\n",
      "\n",
      "Test set: Average loss: 0.00031803398175570907, Accuracy: 64.71000000000001%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.644493\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.640747\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.640375\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.648235\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.644792\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.637833\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.638581\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.631002\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.633757\n",
      "\n",
      "Test set: Average loss: 0.00031659059349401625, Accuracy: 65.08%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.633401\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.639020\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.635225\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.642380\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.642256\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.639264\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.636871\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.649652\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.647506\n",
      "\n",
      "Test set: Average loss: 0.0003150963465114029, Accuracy: 65.25999999999999%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.635763\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.631570\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.623947\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.638940\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.640728\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.628343\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.644398\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.643512\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.630932\n",
      "\n",
      "Test set: Average loss: 0.00031395515306964706, Accuracy: 65.64999999999999%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.200000.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.703890\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.695153\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.682913\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.680375\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.681887\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.674005\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.675989\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.672568\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.671309\n",
      "\n",
      "Test set: Average loss: 0.0003298599906664267, Accuracy: 61.57%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.663589\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.664055\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.659268\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.664412\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.665141\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.662505\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.654405\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.650924\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.649618\n",
      "\n",
      "Test set: Average loss: 0.00032398038692682513, Accuracy: 62.99%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.654103\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.656774\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.659746\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.652471\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.653772\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.648744\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.653156\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.647843\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.654394\n",
      "\n",
      "Test set: Average loss: 0.0003209602429808668, Accuracy: 63.949999999999996%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.652745\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.652776\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.655249\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.643938\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.648165\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.645210\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.646529\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.640504\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.630034\n",
      "\n",
      "Test set: Average loss: 0.0003185359923951584, Accuracy: 64.11%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.648752\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.631669\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.645064\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.647592\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.642907\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.646628\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.638486\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.639634\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.639577\n",
      "\n",
      "Test set: Average loss: 0.0003156165009078473, Accuracy: 65.35%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.627979\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.639233\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.642659\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.628794\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.644246\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.644090\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.639937\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.647198\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.642885\n",
      "\n",
      "Test set: Average loss: 0.00031407132297737165, Accuracy: 65.77%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.637111\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.621716\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.635035\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.631337\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.643351\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.631040\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.631597\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.624561\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.635551\n",
      "\n",
      "Test set: Average loss: 0.00031268188009438483, Accuracy: 66.13%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.635636\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.627100\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.639944\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.634209\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.635153\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.617567\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.630191\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.639576\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.620031\n",
      "\n",
      "Test set: Average loss: 0.00031116849648424777, Accuracy: 66.2%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.639914\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.627897\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.624157\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.626294\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.620567\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.632701\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.624408\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.622150\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.621981\n",
      "\n",
      "Test set: Average loss: 0.00031059627684208973, Accuracy: 66.38%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.632697\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.617169\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.627513\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.623677\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.626653\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.626874\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.627429\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.632497\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.628439\n",
      "\n",
      "Test set: Average loss: 0.00030949761845621023, Accuracy: 66.67%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=0.500000.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.700162\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.689848\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.679389\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.676531\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.662083\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.668097\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.658977\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.655946\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.646571\n",
      "\n",
      "Test set: Average loss: 0.0003236648118142724, Accuracy: 62.519999999999996%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.659573\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.649575\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.644603\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.640985\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.645312\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.644027\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.637752\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.643754\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.631500\n",
      "\n",
      "Test set: Average loss: 0.0003171172909918404, Accuracy: 64.44%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.637211\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.644793\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.637031\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.645451\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.637264\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.635207\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.637372\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.647729\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.638640\n",
      "\n",
      "Test set: Average loss: 0.0003134783679196483, Accuracy: 65.7%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.633208\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.629031\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.633303\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.627235\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.630410\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.633908\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.632962\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.621021\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.628512\n",
      "\n",
      "Test set: Average loss: 0.0003115425671606848, Accuracy: 66.02%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.623018\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.629985\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.624314\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.625341\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.625811\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.627806\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.634413\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.626793\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.625421\n",
      "\n",
      "Test set: Average loss: 0.0003092946256043644, Accuracy: 66.10000000000001%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.627373\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.617701\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.629059\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.622055\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.618259\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.622598\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.627169\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.619363\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.630386\n",
      "\n",
      "Test set: Average loss: 0.0003075461244835952, Accuracy: 67.34%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.619447\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.620987\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.625102\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.627682\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.616977\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.629466\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.613092\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.613659\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.625635\n",
      "\n",
      "Test set: Average loss: 0.0003059849270473658, Accuracy: 67.07%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.623103\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.629491\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.620400\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.616644\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.621317\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.610544\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.620765\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.607666\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.603923\n",
      "\n",
      "Test set: Average loss: 0.0003055528979677729, Accuracy: 67.62%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.620123\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.614615\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.615784\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.611229\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.625168\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.620859\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.615020\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.620306\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.615274\n",
      "\n",
      "Test set: Average loss: 0.00030348945869883694, Accuracy: 68.93%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.615301\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.621714\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.612550\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.614389\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.627162\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.626366\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.611469\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.616682\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.601731\n",
      "\n",
      "Test set: Average loss: 0.0003040755919933402, Accuracy: 68.30000000000001%\n",
      "\n",
      "\n",
      "Training DNN with 200000 data points and SGD lr=1.000000.\n",
      "\n",
      "Train Epoch: 0 [0/180000 (0%)]\tLoss: 0.699506\n",
      "Train Epoch: 0 [20000/180000 (11%)]\tLoss: 0.682755\n",
      "Train Epoch: 0 [40000/180000 (22%)]\tLoss: 0.670904\n",
      "Train Epoch: 0 [60000/180000 (33%)]\tLoss: 0.668226\n",
      "Train Epoch: 0 [80000/180000 (44%)]\tLoss: 0.657415\n",
      "Train Epoch: 0 [100000/180000 (56%)]\tLoss: 0.657178\n",
      "Train Epoch: 0 [120000/180000 (67%)]\tLoss: 0.654718\n",
      "Train Epoch: 0 [140000/180000 (78%)]\tLoss: 0.643201\n",
      "Train Epoch: 0 [160000/180000 (89%)]\tLoss: 0.645756\n",
      "\n",
      "Test set: Average loss: 0.0003190877598926302, Accuracy: 63.73%\n",
      "\n",
      "Train Epoch: 1 [0/180000 (0%)]\tLoss: 0.638043\n",
      "Train Epoch: 1 [20000/180000 (11%)]\tLoss: 0.651570\n",
      "Train Epoch: 1 [40000/180000 (22%)]\tLoss: 0.647943\n",
      "Train Epoch: 1 [60000/180000 (33%)]\tLoss: 0.640824\n",
      "Train Epoch: 1 [80000/180000 (44%)]\tLoss: 0.638169\n",
      "Train Epoch: 1 [100000/180000 (56%)]\tLoss: 0.623102\n",
      "Train Epoch: 1 [120000/180000 (67%)]\tLoss: 0.630604\n",
      "Train Epoch: 1 [140000/180000 (78%)]\tLoss: 0.629285\n",
      "Train Epoch: 1 [160000/180000 (89%)]\tLoss: 0.639647\n",
      "\n",
      "Test set: Average loss: 0.0003126632488298247, Accuracy: 65.72%\n",
      "\n",
      "Train Epoch: 2 [0/180000 (0%)]\tLoss: 0.637301\n",
      "Train Epoch: 2 [20000/180000 (11%)]\tLoss: 0.629069\n",
      "Train Epoch: 2 [40000/180000 (22%)]\tLoss: 0.620044\n",
      "Train Epoch: 2 [60000/180000 (33%)]\tLoss: 0.633076\n",
      "Train Epoch: 2 [80000/180000 (44%)]\tLoss: 0.631354\n",
      "Train Epoch: 2 [100000/180000 (56%)]\tLoss: 0.629692\n",
      "Train Epoch: 2 [120000/180000 (67%)]\tLoss: 0.631724\n",
      "Train Epoch: 2 [140000/180000 (78%)]\tLoss: 0.641905\n",
      "Train Epoch: 2 [160000/180000 (89%)]\tLoss: 0.632956\n",
      "\n",
      "Test set: Average loss: 0.00031193081118056047, Accuracy: 66.32000000000001%\n",
      "\n",
      "Train Epoch: 3 [0/180000 (0%)]\tLoss: 0.632478\n",
      "Train Epoch: 3 [20000/180000 (11%)]\tLoss: 0.632626\n",
      "Train Epoch: 3 [40000/180000 (22%)]\tLoss: 0.626563\n",
      "Train Epoch: 3 [60000/180000 (33%)]\tLoss: 0.626331\n",
      "Train Epoch: 3 [80000/180000 (44%)]\tLoss: 0.629819\n",
      "Train Epoch: 3 [100000/180000 (56%)]\tLoss: 0.615447\n",
      "Train Epoch: 3 [120000/180000 (67%)]\tLoss: 0.628121\n",
      "Train Epoch: 3 [140000/180000 (78%)]\tLoss: 0.630194\n",
      "Train Epoch: 3 [160000/180000 (89%)]\tLoss: 0.623683\n",
      "\n",
      "Test set: Average loss: 0.00030877239413128026, Accuracy: 66.82000000000001%\n",
      "\n",
      "Train Epoch: 4 [0/180000 (0%)]\tLoss: 0.625012\n",
      "Train Epoch: 4 [20000/180000 (11%)]\tLoss: 0.626000\n",
      "Train Epoch: 4 [40000/180000 (22%)]\tLoss: 0.637331\n",
      "Train Epoch: 4 [60000/180000 (33%)]\tLoss: 0.607920\n",
      "Train Epoch: 4 [80000/180000 (44%)]\tLoss: 0.625082\n",
      "Train Epoch: 4 [100000/180000 (56%)]\tLoss: 0.618065\n",
      "Train Epoch: 4 [120000/180000 (67%)]\tLoss: 0.619211\n",
      "Train Epoch: 4 [140000/180000 (78%)]\tLoss: 0.621916\n",
      "Train Epoch: 4 [160000/180000 (89%)]\tLoss: 0.618715\n",
      "\n",
      "Test set: Average loss: 0.0003065285775297068, Accuracy: 67.91%\n",
      "\n",
      "Train Epoch: 5 [0/180000 (0%)]\tLoss: 0.628174\n",
      "Train Epoch: 5 [20000/180000 (11%)]\tLoss: 0.605754\n",
      "Train Epoch: 5 [40000/180000 (22%)]\tLoss: 0.615283\n",
      "Train Epoch: 5 [60000/180000 (33%)]\tLoss: 0.635209\n",
      "Train Epoch: 5 [80000/180000 (44%)]\tLoss: 0.609649\n",
      "Train Epoch: 5 [100000/180000 (56%)]\tLoss: 0.614812\n",
      "Train Epoch: 5 [120000/180000 (67%)]\tLoss: 0.634115\n",
      "Train Epoch: 5 [140000/180000 (78%)]\tLoss: 0.620344\n",
      "Train Epoch: 5 [160000/180000 (89%)]\tLoss: 0.620133\n",
      "\n",
      "Test set: Average loss: 0.000305832069490808, Accuracy: 67.42%\n",
      "\n",
      "Train Epoch: 6 [0/180000 (0%)]\tLoss: 0.619154\n",
      "Train Epoch: 6 [20000/180000 (11%)]\tLoss: 0.615401\n",
      "Train Epoch: 6 [40000/180000 (22%)]\tLoss: 0.620647\n",
      "Train Epoch: 6 [60000/180000 (33%)]\tLoss: 0.616310\n",
      "Train Epoch: 6 [80000/180000 (44%)]\tLoss: 0.610992\n",
      "Train Epoch: 6 [100000/180000 (56%)]\tLoss: 0.620485\n",
      "Train Epoch: 6 [120000/180000 (67%)]\tLoss: 0.607264\n",
      "Train Epoch: 6 [140000/180000 (78%)]\tLoss: 0.606727\n",
      "Train Epoch: 6 [160000/180000 (89%)]\tLoss: 0.615972\n",
      "\n",
      "Test set: Average loss: 0.00030452557215388334, Accuracy: 68.08%\n",
      "\n",
      "Train Epoch: 7 [0/180000 (0%)]\tLoss: 0.616024\n",
      "Train Epoch: 7 [20000/180000 (11%)]\tLoss: 0.607679\n",
      "Train Epoch: 7 [40000/180000 (22%)]\tLoss: 0.620621\n",
      "Train Epoch: 7 [60000/180000 (33%)]\tLoss: 0.600653\n",
      "Train Epoch: 7 [80000/180000 (44%)]\tLoss: 0.620752\n",
      "Train Epoch: 7 [100000/180000 (56%)]\tLoss: 0.608143\n",
      "Train Epoch: 7 [120000/180000 (67%)]\tLoss: 0.611895\n",
      "Train Epoch: 7 [140000/180000 (78%)]\tLoss: 0.618127\n",
      "Train Epoch: 7 [160000/180000 (89%)]\tLoss: 0.623760\n",
      "\n",
      "Test set: Average loss: 0.00030345193879093317, Accuracy: 68.58%\n",
      "\n",
      "Train Epoch: 8 [0/180000 (0%)]\tLoss: 0.624070\n",
      "Train Epoch: 8 [20000/180000 (11%)]\tLoss: 0.608591\n",
      "Train Epoch: 8 [40000/180000 (22%)]\tLoss: 0.612716\n",
      "Train Epoch: 8 [60000/180000 (33%)]\tLoss: 0.611359\n",
      "Train Epoch: 8 [80000/180000 (44%)]\tLoss: 0.611932\n",
      "Train Epoch: 8 [100000/180000 (56%)]\tLoss: 0.606687\n",
      "Train Epoch: 8 [120000/180000 (67%)]\tLoss: 0.614583\n",
      "Train Epoch: 8 [140000/180000 (78%)]\tLoss: 0.611881\n",
      "Train Epoch: 8 [160000/180000 (89%)]\tLoss: 0.616338\n",
      "\n",
      "Test set: Average loss: 0.0003026603979785718, Accuracy: 68.74%\n",
      "\n",
      "Train Epoch: 9 [0/180000 (0%)]\tLoss: 0.630917\n",
      "Train Epoch: 9 [20000/180000 (11%)]\tLoss: 0.617102\n",
      "Train Epoch: 9 [40000/180000 (22%)]\tLoss: 0.601824\n",
      "Train Epoch: 9 [60000/180000 (33%)]\tLoss: 0.608922\n",
      "Train Epoch: 9 [80000/180000 (44%)]\tLoss: 0.603937\n",
      "Train Epoch: 9 [100000/180000 (56%)]\tLoss: 0.583100\n",
      "Train Epoch: 9 [120000/180000 (67%)]\tLoss: 0.609539\n",
      "Train Epoch: 9 [140000/180000 (78%)]\tLoss: 0.608307\n",
      "Train Epoch: 9 [160000/180000 (89%)]\tLoss: 0.615088\n",
      "\n",
      "Test set: Average loss: 0.0003023893937796759, Accuracy: 68.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmcb1\\AppData\\Local\\Temp\\ipykernel_5200\\368025010.py:334: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(['']+x)\n",
      "C:\\Users\\pmcb1\\AppData\\Local\\Temp\\ipykernel_5200\\368025010.py:335: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(['']+y)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys\n",
    "import time\n",
    "\n",
    "#definir a classe dos dados\n",
    "class HIGGS(torch.utils.data.Dataset):\n",
    "    def __init__ (self, data_size, train=None, high_feats=None, validation=True):\n",
    "        features = [\"HIGGS\",\"lepton  pT\", \"lepton  eta\", \"lepton  phi\", \"missing energy magnitude\", \"missing energy phi\", \"jet 1 pt\", \"jet 1 eta\", \"jet 1 phi\", \"jet 1 b-tag\", \"jet 2 pt\", \"jet 2 eta\",\n",
    "                     \"jet 2 phi\", \"jet 2 b-tag\", \"jet 3 pt\", \"jet 3 eta\", \"jet 3 phi\", \"jet 3 b-tag\", \"jet 4 pt\", \"jet 4 eta\", \"jet 4 phi\", \"jet 4 b-tag\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]\n",
    "\n",
    "        low_level_features = [\"lepton  pT\", \"lepton  eta\", \"lepton  phi\", \"missing energy magnitude\", \"missing energy phi\", \"jet 1 pt\", \"jet 1 eta\", \"jet 1 phi\", \"jet 1 b-tag\", \"jet 2 pt\", \"jet 2 eta\",\n",
    "                               \"jet 2 phi\", \"jet 2 b-tag\", \"jet 3 pt\", \"jet 3 eta\", \"jet 3 phi\", \"jet 3 b-tag\", \"jet 4 pt\", \"jet 4 eta\", \"jet 4 phi\", \"jet 4 b-tag\"]\n",
    "        \n",
    "        high_level_features = [\"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]\n",
    "\n",
    "        file = \"HIGGS.csv\"\n",
    "        root = os.path.expanduser(\"~\")+\"\\\\Desktop\\\\Ciência de Dados\\\\\"\n",
    "        datapoints = pd.read_csv(root+file, header=None, nrows=data_size, engine=\"python\")\n",
    "        datapoints.columns = features\n",
    "        X = datapoints[features[1:]]\n",
    "        Y = datapoints[\"HIGGS\"]\n",
    "\n",
    "        if validation is True:\n",
    "            training_size = int(0.9 * data_size) #90% para treino\n",
    "            valid_size = int(0.05 * data_size)#5% para validação = treino\n",
    "\n",
    "            if train is True:\n",
    "                X = X[:training_size]\n",
    "                Y = Y[:training_size]\n",
    "                print(\"Training sample size = {}\".format(training_size))\n",
    "            elif train is None:\n",
    "                X = X[training_size:training_size+valid_size]\n",
    "                Y = Y[training_size:training_size+valid_size]\n",
    "                print(\"Validation sample size = {}\".format(valid_size))\n",
    "            elif train is False:\n",
    "                X = X[training_size+valid_size:]\n",
    "                Y = Y[training_size+valid_size:]\n",
    "                print(\"Testing sample size = {}\".format(valid_size))\n",
    "\n",
    "\n",
    "            #Já a pensar na dimensionality reduction, vou montar 3 situações: low+high feats, low feats only, high feats only\n",
    "            if high_feats is None:\n",
    "                self.data = (X.values.astype(np.float64), Y.values.astype(int))\n",
    "                print(\"Using all features\")\n",
    "            elif high_feats is False:\n",
    "                self.data = (X[low_level_features].values.astype(np.float64), Y.values.astype(int))\n",
    "                print(\"Using low level features only\")\n",
    "            elif high_feats is True:\n",
    "                self.data = (X[high_level_features].values.astype(np.float64), Y.values.astype(int))\n",
    "                print(\"Using high level features only\")\n",
    "\n",
    "            self.train = train\n",
    "\n",
    "        if validation is False:\n",
    "            training_size = 1800000 #int(0.9 * data_size)\n",
    "            test_size = 200000 \n",
    "\n",
    "            if train is True:\n",
    "                X = X[:training_size]\n",
    "                Y = Y[:training_size]\n",
    "                print(\"Training sample size = {}\".format(training_size))\n",
    "            elif train is False:\n",
    "                X = X[-test_size:]\n",
    "                Y = Y[-test_size:]\n",
    "                print(\"Testing sample size = {}\".format(test_size))\n",
    "\n",
    "            if high_feats is None:\n",
    "                self.data = (X.values.astype(np.float64), Y.values.astype(int))\n",
    "                print(\"Using all features\")\n",
    "            elif high_feats is False:\n",
    "                self.data = (X[low_level_features].values.astype(np.float64), Y.values.astype(int))\n",
    "                print(\"Using low level features only\")\n",
    "            elif high_feats is True:\n",
    "                self.data = (X[high_level_features].values.astype(np.float64), Y.values.astype(int))\n",
    "                print(\"Using high level features only\")\n",
    "\n",
    "            self.train = train\n",
    "\n",
    "\n",
    "\n",
    "    #Esta classe definida pelo torch requer que demos override a duas funções, uma que devolve o tamanho do conjunto de dados e outra que acede a cada valor (sample) deste conjunto\n",
    "    def __len__ (self):\n",
    "        return len(self.data[1]) #é o tamanho do Y (numero de linhas)\n",
    "        \n",
    "    def __getitem__ (self, i):\n",
    "        features = torch.tensor(self.data[0][i], dtype=torch.float64)\n",
    "        label = torch.tensor(self.data[1][i], dtype=torch.long)\n",
    "        sample = (features, label)\n",
    "\n",
    "        return sample\n",
    "\n",
    "#A próxima função é responsável por \"carregar\" cada dataset (treino e test) para que estes sejam utilizados pela DNN\n",
    "#Esta função aceita como argumento \n",
    "        \n",
    "def loaders(args):\n",
    "    if args.validation is True:\n",
    "        train_loader = torch.utils.data.DataLoader(HIGGS(args.data_size, train=True, high_feats=args.high_feats, validation=args.validation), batch_size=args.batch_size, shuffle=True)#O \"batch_size\" é essencialmente número de eventos que são utilizados de cada vez num epoch (numero de updates em 1 epoch = training_size/batch_size)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(HIGGS(args.data_size, train=False, high_feats=args.high_feats, validation=args.validation), batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        valid_loader = torch.utils.data.DataLoader(HIGGS(args.data_size, train=None, high_feats=args.high_feats, validation=args.validation), batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, test_loader, valid_loader\n",
    "    elif args.validation is False:\n",
    "        train_loader = torch.utils.data.DataLoader(HIGGS(args.data_size, train=True, high_feats=args.high_feats, validation=args.validation), batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(HIGGS(args.data_size, train=False, high_feats=args.high_feats, validation=args.validation), batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "#Uma vez procesados os dados, vamos definir a classe para o modelo de NN\n",
    "class model(nn.Module):\n",
    "    #Esta função define a composição da NN em termos das suas camadas e do tipo de operação (neste caso será linear) que aplica ao input\n",
    "    def __init__ (self, high_feats=None):\n",
    "        super(model, self).__init__() #Inicializamos a \"super\" para que o modelo adquira os atríbutos do \"nn.Module\"\n",
    "\n",
    "        layers = [] #Camadas de neurónios da DNN\n",
    "        #Aqui será interessante tentar ver a dependencia do número de neurónios\n",
    "        if high_feats is None:         \n",
    "            layers.append(nn.Linear(28,200).double()) #Neste caso usamos todas as features=28, então definimos a primeira camada como sendo composta por 28 neurónios (1 para cada variável de input) que processam o input como uma transformação linear do tipo y=wx+b e enviam o output para 200 neuronios\n",
    "        elif high_feats is False:\n",
    "            layers.append(nn.Linear(21,200).double()) #Só low level features\n",
    "        elif high_feats is True:\n",
    "            layers.append(nn.Linear(7,200).double()) #Só high level features\n",
    "\n",
    "        \n",
    "        layers.append(nn.Linear(200,100).double())\n",
    "        layers.append(nn.Linear(100,2).double())        #Outra hipótese a investigar é a de um único neurónio de output pois apenas me importa qual a probabilidade de ser sinal (a prob de fundo = 1-prob_sinal)\n",
    "        self.model_layers = nn.ModuleList(layers)\n",
    "    \n",
    "    #Esta função carateriza o funcionamente da NN, mais especificamente como esta passa os dados entre as suas camadas (exceto a última)\n",
    "    def forward(self, x):\n",
    "        for layer in self.model_layers[:-1]:\n",
    "            x = F.relu(layer(x)) #relu -> Rectified linear unit activation function que devolve o valor 0 se o output da layer for negativo, ou devolve o seu valor caso o output seja positivo\n",
    "            x = F.dropout(x, training=self.training)#Regularização através de \"dropout\" para prevenir \"overfitting\", definindo aleatóriamente alguns inputs como 0 durante o TREINO. Isto apenas acontece no treino e é garantido pela condição \"training=self.training\", pois \"self.training = True\" apenas quando a DNN está no modo \".train()\"\n",
    "        #Após passar pelas diversas camadas passamos o output pela última camada sem aplicar \"ReLU\" ou \"dropout\", sendo esta a camada que produz o output final do modelo\n",
    "        x = self.model_layers[-1](x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "#Uma vez definido o nosso modelo, passamos a construir a função que define o comportamento do modelo na fase de treino e de teste\n",
    "def evaluate_model(args, train_loader, test_loader, valid_loader):\n",
    "    #Primeiro inicializamos o modelo\n",
    "    DNN = model(high_feats=args.high_feats) \n",
    "\n",
    "    #De seguida escolhemos a loss function (Cross-Entropy Loss)\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "    #Escolhemos também o otimizador (Stochastic Gradient Descent)\n",
    "    optimizer = optim.SGD(DNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    #Agora definimos a função de treino que toma como input \"epoch\", isto é o número de iterações que a DNN faz durante o treino \n",
    "    def train(epoch):#Os \"epoch\" são o número de vezes que o modelo percorre o \"training set\"\n",
    "        DNN.train()#Começamos por colocar o modelo em modo de treino, o que seleciona \"self.training=True\" e ativa o \"dropout\" na \"Forward Pass\"\n",
    "        for event, (data, label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()#Restora os gradientes dos parametros a zero\n",
    "            output = DNN(data)#Output proveniente do \"Forward Pass\" na forma de 2 valores entre 0 e 1 com soma igual a 1, cada um correpsondente à probabilidade do evento ser da classe correspondente ao seu índice na lista \"output\"\n",
    "            #O output sai no formato de um ntuple de n = batch_size; se batch_size = 2 -> output = ([p1,p2],[p3,p4])\n",
    "            loss = criteria(output, label)#Calcula a perda entre a previsão do modelo (output) e o valor real (label)\n",
    "            loss.backward()#Realiza a backpropagation das perdas\n",
    "            optimizer.step()#Atualiza os parametros com os gradientes computados\n",
    "            #print(\"Esta é a forma do output: {}\".format(output))\n",
    "\n",
    "            #É do nosso interesse visualizar se o treino está a ser bem efetuado através da visualização da perda (o treino deve minimizar as perdas)\n",
    "            if event % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, event * len(data), len(train_loader.dataset),\n",
    "                    100. * event / len(train_loader), loss.item()))\n",
    "                \n",
    "        return loss.item()\n",
    "    \n",
    "    #Definimos de seguida a função de teste, na qual vamos verificar o resultado do treino\n",
    "    def test():\n",
    "        DNN.eval()#Começamos por colocar o modelo em modo de teste, o que seleciona \"self.training=False\" e desativa o \"dropout\" na \"Forward Pass\"\n",
    "\n",
    "        #Para quantificarmos o treino, convém calcularmos algumas propriedades do modelo, como a \"accuracy\" e a \"average loss\"\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        #Guardamos aqui as probabilidades dos eventos serem sinal, na lista à qual o seu label real corresponde\n",
    "        valid_signal = []\n",
    "        valid_background = []\n",
    "\n",
    "        test_signal = []\n",
    "        test_background = []\n",
    "\n",
    "        with torch.no_grad():#Utilizamos esta função para que o Pytorch não compute os gradientes pois não pretendemos atualizar os parametros do modelo durante a fase de teste\n",
    "            for data, label in valid_loader:\n",
    "                output = DNN(data)\n",
    "                for i in range(len(label)):\n",
    "                    if label[i] == 0:\n",
    "                        valid_background.append(output[i,1].item())#Vamos apenas guardar a segunda probabilidade \"indice = 1\" (prob de ser sinal) para montar a test statistic\n",
    "                    if label[i] == 1:\n",
    "                        valid_signal.append(output[i,1].item())\n",
    "            \n",
    "            for data, label in test_loader:\n",
    "                output = DNN(data)\n",
    "                for i in range(len(label)):\n",
    "                    if label[i] == 0:\n",
    "                        test_background.append(output[i,1].item())#Vamos apenas guardar a segunda probabilidade \"indice = 1\" (prob de ser sinal) para montar a test statistic\n",
    "                    if label[i] == 1:\n",
    "                        test_signal.append(output[i,1].item())\n",
    "                test_loss += criteria(output, label).item()#Soma da perda em cada evento\n",
    "                prediction = output.argmax(dim=1, keepdim=True)#A previsão é igual ao índice do \"output\" correspondente ao maior valor, pois este é a probabilidade do evento ser dessa classe (0 ou 1)\n",
    "                correct += prediction.eq(label.view_as(prediction)).sum().item()#Aqui a previsão é comparada com o \"label\" do evento e caso sejam iguais é adicionada uma contagem ao número de previsões corretas (\"correct\")\n",
    "        \n",
    "        average_loss = test_loss / len(test_loader.dataset)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        \n",
    "        #Print apenas por razões de visualização do corrido a correr\n",
    "        print(\"\\nTest set: Average loss: {}, Accuracy: {}%\\n\".format(average_loss, 100 * accuracy))\n",
    "        \n",
    "        return average_loss, accuracy, valid_signal, valid_background, test_signal, test_background\n",
    "    \n",
    "    def test_no_label():\n",
    "        \n",
    "        DNN.eval()\n",
    "\n",
    "        test_probabilities = []\n",
    "\n",
    "        with torch.no_grad():#Utilizamos esta função para que o Pytorch não compute os gradientes pois não pretendemos atualizar os parametros do modelo durante a fase de teste\n",
    "            for data, label in test_loader:\n",
    "                output = DNN(data)\n",
    "                for i in range(len(output)):\n",
    "                    test_probabilities.append(output[i,1].item())#Vamos apenas guardar a segunda probabilidade \"indice = 1\" (prob de ser sinal) para montar a test statistic\n",
    "    \n",
    "        return test_probabilities\n",
    "    \n",
    "    #Vamos agora criar 3 arrays para guardar as perdas durante o treino e teste e ainda a accuracy do teste\n",
    "    train_loss = np.zeros((args.epochs,))\n",
    "    test_loss = np.zeros_like(train_loss)\n",
    "    test_accuracy = np.zeros_like(train_loss)\n",
    "\n",
    "    #valid_signal = []\n",
    "    #valid_background = []\n",
    "    #test_signal = []\n",
    "    #test_background = []\n",
    "\n",
    "    st = time.time()#starting time\n",
    "    elapsed_time = []#final time\n",
    "\n",
    "    validation = args.validation\n",
    "    #Vamos então fazer a iteração sobre cada epoch e guardar os valores das perdas e da accuracy nas respetivas listas\n",
    "    if validation is True:\n",
    "        for epoch in range(args.epochs):\n",
    "            train_loss[epoch] = train(epoch)\n",
    "            test_loss[epoch], test_accuracy[epoch], valid_signal, valid_background, test_signal, test_background = test()\n",
    "            elapsed_time.append(time.time()-st)\n",
    "\n",
    "        return test_loss, test_accuracy, elapsed_time, valid_signal, valid_background, test_signal, test_background\n",
    "    \n",
    "    elif validation is False:\n",
    "        for epoch in range(args.epochs):\n",
    "            train_loss[epoch] = train(epoch)\n",
    "            test_probabilities = test_no_label()\n",
    "            elapsed_time.append(time.time()-st)\n",
    "\n",
    "        return test_probabilities\n",
    "\n",
    "#Agora que definimos o treino e teste da DNN, vamos definir a otimização do nosso modelo onde vamos investigar o impacto de diversos parâmetros\n",
    "def optim_datasize(args):\n",
    "    data_sizes = [1000, 10000, 100000, 200000] #Estudo do impacto do data_size\n",
    "    learning_rates = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1]#Estudo do impacto do learning_rate\n",
    "\n",
    "    #Criamos listas para guardar a perda e a accuracy associada a cada data_size e learning_rate\n",
    "    test_loss = np.zeros((len(data_sizes), len(learning_rates)), dtype=np.float64)\n",
    "    test_accuracy = np.zeros_like(test_loss)\n",
    "\n",
    "    #Agora realizamos a \"grid_search\" que itera sobre todos os eventos\n",
    "    for i, data_size in enumerate(data_sizes):\n",
    "        #Atualizamos os parâmetros das funções\n",
    "        args.data_size = data_size\n",
    "        args.batch_size = int(0.01*data_size)\n",
    "\n",
    "        #Carregamos os dados de treino e teste\n",
    "        train_loader, test_loader, valid_loader = loaders(args)\n",
    "\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            #Atualizar o learning_rate\n",
    "            args.lr = lr\n",
    "\n",
    "            #Print para sabermos o que estamos a fazer\n",
    "            print(\"\\nTraining DNN with %5d data points and SGD lr=%0.6f.\\n\"%(data_size,lr))\n",
    "\n",
    "            #Agora fazemos o treino e teste do modelo e guardamos a perda e a accuracy da fase de teste\n",
    "            x, y, q, w, e, r, t = evaluate_model(args, train_loader, test_loader, valid_loader)\n",
    "            test_loss[i, j] = x[-1] #Loss após todos os epochs\n",
    "            test_accuracy[i, j] = y[-1] #Accuracy após todos os epochs\n",
    "\n",
    "    #print(test_loss)\n",
    "    #print(test_loss_epoch[1])\n",
    "    \n",
    "    #Com esta informação montamos um gráfico da accuracy em função do learning_rate e do data_size\n",
    "    plot_datasize(learning_rates, data_sizes, test_accuracy)\n",
    "\n",
    "\n",
    "    #Seria possível realizar ainda mais plots para avaliar outras variavéis\n",
    "\n",
    "#talvez tenha de trocar a ordem de x e y nesta função pq em cima quando chamo esta função eu troco o l_r de posição com data_size (no codigo original era plot(learning_rate, data_size, test_accuracy))\n",
    "#eu fiz a troca e voltei ao estado original\n",
    "def plot_datasize(x, y, test_accuracy):\n",
    "    fontsize=16\n",
    "\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(test_accuracy, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('accuracy (%)',rotation=90,fontsize=fontsize)\n",
    "    cbar.set_ticks([0,.2,.4,.6,.8,1.0])\n",
    "    cbar.set_ticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"${0:.1f}\\\\%$\".format( 100*test_accuracy[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis vaues to to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\mathrm{Learning\\\\ Rate}$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\mathrm{Dataset\\\\ Size}$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #f2 = plt.figure(2)\n",
    "    #ax2 = fig.add_subplot(111)\n",
    "    #cax2 = ax2.matshow(test_accuracy, interpolation='nearest', vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "         \n",
    "def optim_lr(args):\n",
    "    learning_rates = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1]\n",
    "  \n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    elapsed_time = []\n",
    "\n",
    "    for j, lr in enumerate(learning_rates):\n",
    "        args.lr = lr\n",
    "        args.batch_size = int(0.01*args.data_size)\n",
    "\n",
    "        train_loader, test_loader = loaders(args)\n",
    "        print(\"\\nTraining DNN with SGD lr=%0.6f.\\n\"%(lr))\n",
    "\n",
    "        x , y , z = evaluate_model(args, train_loader, test_loader)\n",
    "        test_loss.append(x)#len = epochs\n",
    "        test_accuracy.append(y)#len = epochs\n",
    "        elapsed_time.append(z)#len = epochs\n",
    "\n",
    "    plot_lr(test_loss, learning_rates, test_accuracy, elapsed_time)\n",
    "\n",
    "def plot_lr(loss, lr, accuracy, elapsed_time):\n",
    "    #for i, y in enumerate(loss):\n",
    "    #    epochs = [e for e in range(1, len(loss[i])+1)]\n",
    "    #    plt.plot(epochs, y, label=\"Lr={}\".format(lr[i]))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Average Test Loss\")\n",
    "    #plt.title(\"Data size = {} and batch size = {}\".format(args.data_size, int(0.01*args.data_size)))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #for i, y in enumerate(accuracy):\n",
    "    #    epochs = [e for e in range(1, len(accuracy[i])+1)]\n",
    "    #    plt.plot(epochs, y, label=\"Lr={}\".format(lr[i]))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Data size = {} and batch size = {}\".format(args.data_size, int(0.01*args.data_size)))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    for i, y in enumerate(elapsed_time):\n",
    "        epochs = [e for e in range(1, len(elapsed_time[i])+1)]\n",
    "        plt.plot(epochs, y, label=\"Lr={}\".format(lr[i]))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Elapsed time\")\n",
    "    plt.title(\"Data size = {} and batch size = {}\".format(args.data_size, int(0.01*args.data_size)))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def optim_batch(args):\n",
    "    batch_sizes = [0.1, 0.01, 0.001]\n",
    "\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    elapsed_time = []\n",
    "\n",
    "    for i, batch_size in enumerate(batch_sizes):\n",
    "        args.batch_size = int(batch_size*args.data_size)\n",
    "\n",
    "        train_loader, test_loader = loaders(args)\n",
    "        print(\"\\nTraining DNN with Batch_size=%0.6f.\\n\"%(int(batch_size*args.data_size)))\n",
    "\n",
    "        x, y, z = evaluate_model(args, train_loader, test_loader)\n",
    "        test_loss.append(x)\n",
    "        test_accuracy.append(y)\n",
    "        elapsed_time.append(z)\n",
    "    \n",
    "    plot_batch(test_loss, batch_sizes, test_accuracy, elapsed_time)\n",
    "\n",
    "def plot_batch(loss, batches, accuracy, elapsed_time):\n",
    "    #for i, y in enumerate(loss):\n",
    "    #    epochs = [e for e in range(1, len(loss[i])+1)]\n",
    "    #    plt.plot(epochs, y, label=\"Batch size={}\".format(batches[i]))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Average Test Loss\")\n",
    "    #plt.title(\"Data size = {} and learning rate = {}\".format(args.data_size, args.lr))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #Para este gráfico retirei o batch_size=0.0001 pois pelo gráfico de cima vimos que tem uma grande perda\n",
    "    #for i, y in enumerate(accuracy):\n",
    "    #    epochs = [e for e in range(1, len(accuracy[i])+1)]\n",
    "    #    plt.plot(epochs, y, label=\"Batch size={}\".format(batches[i]))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Data size = {} and learning rate = {}\".format(args.data_size, args.lr))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    for i, y in enumerate(elapsed_time):\n",
    "        epochs = [e for e in range(1, len(elapsed_time[i])+1)]\n",
    "        plt.plot(epochs, y, label=\"Batch size={}\".format(batches[i]))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Elapsed time\")\n",
    "    plt.title(\"Data size = {} and learning rate = {}\".format(args.data_size, args.lr))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def optim_momentum(args):\n",
    "    momenta = [ 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    elapsed_time = []\n",
    "\n",
    "    for i, momentum in enumerate(momenta):\n",
    "        args.batch_size = int(0.01*args.data_size)\n",
    "\n",
    "        train_loader, test_loader = loaders(args)\n",
    "        print(\"\\nTraining DNN with Momentum=%0.6f.\\n\"%(momentum))\n",
    "\n",
    "        x, y, z = evaluate_model(args, train_loader, test_loader)\n",
    "        test_loss.append(x)\n",
    "        test_accuracy.append(y)\n",
    "        elapsed_time.append(z)\n",
    "\n",
    "    plot_momentum(test_loss, momenta, test_accuracy, elapsed_time)\n",
    "\n",
    "def plot_momentum(loss, momenta, accuracy, elapsed_time):\n",
    "    #for i, y in enumerate(loss):\n",
    "    #    epochs = [e for e in range(1, len(loss[i])+1)]\n",
    "    #    plt.plot(epochs, y, label=\"Momentum={}\".format(momenta[i]))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Average Test Loss\")\n",
    "    #plt.title(\"Data size = {}, learning rate = {}, batch size = {}\".format(args.data_size, args.lr, int(0.01*args.data_size)))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #for i, y in enumerate(accuracy):\n",
    "    #    epochs = [e for e in range(1, len(accuracy[i])+1)]\n",
    "    #    plt.plot(epochs, y, label=\"Momentum={}\".format(momenta[i]))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Data size = {}, learning rate = {}, batch size = {}\".format(args.data_size, args.lr, int(0.01*args.data_size)))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    for i, y in enumerate(elapsed_time):\n",
    "        epochs = [e for e in range(1, len(elapsed_time[i])+1)]\n",
    "        plt.plot(epochs, y, label=\"Momentum={}\".format(momenta[i]))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Elapsed time\")\n",
    "    plt.title(\"Data size = {}, learning rate = {}, batch size = {}\".format(args.data_size, args.lr, int(0.01*args.data_size)))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def optim_epoch(args):\n",
    "\n",
    "    train_loader, test_loader, valid_loader = loaders(args)\n",
    "    print(\"\\nTraining DNN with Epoch=%0.6f.\\n\"%(args.epochs))\n",
    "\n",
    "    test_loss, test_accuracy, elapsed_time = evaluate_model(args, train_loader, test_loader, valid_loader)\n",
    "\n",
    "    plot_epoch(test_loss, args.epochs, test_accuracy, elapsed_time)\n",
    "\n",
    "def plot_epoch(loss, epochs, accuracy, elapsed_time):\n",
    "    \n",
    "    #x = [e for e in range(1, len(loss)+1)]\n",
    "    #plt.plot(x, loss, label=\"Data size={}, Lr={}, Batch size={}, Momentum={}\".format(args.data_size, args.lr, int(0.01*args.data_size), args.momentum))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Average Test Loss\")\n",
    "    #plt.title(\"Epoch optimization\")\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    #x = [e for e in range(1, len(accuracy)+1)]\n",
    "    #plt.plot(x, accuracy, label=\"Data size={}, Lr={}, Batch size={}, Momentum={}\".format(args.data_size, args.lr, int(0.01*args.data_size), args.momentum))\n",
    "    #plt.xlabel(\"Epoch\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.title(\"Epoch optimization\")\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    x = [e for e in range(1, len(elapsed_time)+1)]\n",
    "    plt.plot(x, elapsed_time, label=\"Data size={}, Lr={}, Batch size={}, Momentum={}\".format(args.data_size, args.lr, int(0.01*args.data_size), args.momentum))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Elapsed time\")\n",
    "    plt.title(\"Epoch optimization\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def validation(args):\n",
    "    args.validation = True\n",
    "    features = [True]#, False, None]\n",
    "\n",
    "    signal_efficiency = []\n",
    "    background_rejection = []\n",
    "    significance = []\n",
    "\n",
    "    for i in features:\n",
    "        args.high_feats = i\n",
    "        train_loader, test_loader, valid_loader = loaders(args)\n",
    "        print(\"\\nTraining DNN with dataset size = {}, batch size = {}, learning rate = {}, momentum = {} and epochs = {}.\\n\".format(args.data_size, args.batch_size, args.lr, args.momentum, args.epochs))\n",
    "        \n",
    "        test_loss, test_accuracy, elapsed_time, valid_signal, valid_background, test_signal, test_background = evaluate_model(args, train_loader, test_loader, valid_loader)\n",
    "\n",
    "        bins = args.bins\n",
    "        plot_validation(valid_signal, valid_background, test_signal, test_background, bins)\n",
    "\n",
    "        #Com base nos histogramas da \"test statistic\" vamos aplicar um corte arbitrário em p=0.5 para fazer a validação\n",
    "        #p = 0.5\n",
    "\n",
    "        step = 1/bins\n",
    "\n",
    "        p = np.arange(0, 1, step)\n",
    "\n",
    "        s_efficiency = []\n",
    "        b_rejection = []\n",
    "\n",
    "        for i in p:\n",
    "\n",
    "            #valid_signal_r = 0\n",
    "            #valid_signal_l = 0\n",
    "            #valid_background_r = 0\n",
    "            #valid_background_l = 0\n",
    "\n",
    "            test_signal_r = 0\n",
    "            #test_signal_l = 0\n",
    "            test_background_r = 0\n",
    "            test_background_l = 0\n",
    "            #for i in valid_signal:\n",
    "            #    if i < p:\n",
    "            #        valid_signal_l += 1\n",
    "            #    if i > p:\n",
    "            #        valid_signal_r += 1\n",
    "\n",
    "            #for i in valid_background:\n",
    "            #    if i < p:\n",
    "            #        valid_background_l += 1\n",
    "            #    if i > p:\n",
    "            #        valid_background_r += 1\n",
    "\n",
    "            for j in test_signal:\n",
    "                #if j < p:\n",
    "                #    test_signal_l += 1\n",
    "                if j > i:\n",
    "                    test_signal_r += 1\n",
    "\n",
    "            for j in test_background:\n",
    "                #if j < i:\n",
    "                #    test_background_l += 1\n",
    "                if j > i:\n",
    "                    test_background_r += 1\n",
    "\n",
    "            test_background_r = 326*test_background_r\n",
    "            #test_background_l = 326*test_background_l\n",
    "            #s_efficiency.append(test_signal_r/len(test_signal))\n",
    "            #b_rejection.append(test_background_l/len(test_background))\n",
    "            significance.append(test_signal_r/np.sqrt(test_signal_r+test_background_r))\n",
    "\n",
    "        #signal_efficiency.append(s_efficiency)\n",
    "        #background_rejection.append(b_rejection)\n",
    "\n",
    "    print(significance)\n",
    "    significance_max = significance.index(max(significance))\n",
    "    print(significance_max)\n",
    "    print(\"tamanho da significance list = {}\".format(len(significance)))\n",
    "    print(\"tamanho das probabilidades = {}\".format(len(p)))\n",
    "    best_cut = p[significance_max]\n",
    "    print(\"\\nThe cut that maximizes significance is p = {}\\n\".format(best_cut))\n",
    "\n",
    "    plot_significance(significance, p)\n",
    "    #plot_efficiency_rejection(s_efficiency, b_rejection)\n",
    "    #plot_efficiency_rejection(signal_efficiency, background_rejection)\n",
    "\n",
    "\n",
    "    #print(\"\\nSinal à esquerda do corte no set de validação = {}\\n\".format(valid_signal_l))\n",
    "    #print(\"\\nSinal à direita do corte no set de validação = {}\\n\".format(valid_signal_r))\n",
    "    #print(\"\\nBackground à esquerda do corte no set de validação = {}\\n\".format(valid_background_l))\n",
    "    #print(\"\\nBackground à direita do corte no set de validação = {}\\n\".format(valid_background_r))\n",
    "\n",
    "    #print(\"\\nSinal à esquerda do corte no set de teste = {}\\n\".format(test_signal_l))\n",
    "    #print(\"\\nSinal à direita do corte no set de teste = {}\\n\".format(test_signal_r))\n",
    "    #print(\"\\nBackground à esquerda do corte no set de teste = {}\\n\".format(test_background_l))\n",
    "    #print(\"\\nBackground à direita do corte no set de teste = {}\\n\".format(test_background_r))\n",
    "\n",
    "    #print(\"\\nTotal à esquerda do corte no set de validação = {} +- {}\\n\".format(valid_signal_l+valid_background_l, int(np.sqrt(valid_signal_l+valid_background_l))))\n",
    "    #print(\"\\nTotal à direita do corte no set de validação = {} +- {}\\n\".format(valid_signal_r+valid_background_r, int(np.sqrt(valid_signal_r+valid_background_r))))\n",
    "    #print(\"\\nTotal à esquerda do corte no set de teste = {} +- {}\\n\".format(test_signal_l+test_background_l, int(np.sqrt(test_signal_l+test_background_l))))\n",
    "    #print(\"\\nTotal à direita do corte no set de teste = {} +- {}\\n\".format(test_signal_r+test_background_r, int(np.sqrt(test_signal_r+test_background_r))))\n",
    "\n",
    "def plot_validation(valid_signal, valid_background, test_signal, test_background, bins):\n",
    "    \n",
    "    plt.hist(valid_signal, bins, alpha = 0.5, label = \"Signal\")\n",
    "    plt.hist(valid_background, bins, alpha = 0.5, label = \"Background\")\n",
    "    plt.xlabel(\"Probability of signal\")\n",
    "    plt.ylabel(\"Number of events\")\n",
    "    plt.title(\"Validation set\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(test_signal, bins, alpha = 0.5, label = \"Signal\")\n",
    "    plt.hist(test_background, bins, alpha = 0.5, label = \"Background\")\n",
    "    plt.xlabel(\"Probability of signal\")\n",
    "    plt.ylabel(\"Number of events\")\n",
    "    plt.title(\"Testing set\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_efficiency_rejection(signal_efficiency, background_rejection):\n",
    "\n",
    "    #plt.plot(signal_efficiency, background_rejection, label = \"High features only\")\n",
    "\n",
    "    plt.plot(signal_efficiency[0], background_rejection[0], label = \"High features only\")\n",
    "    plt.plot(signal_efficiency[1], background_rejection[1], label = \"Low features only\")\n",
    "    plt.plot(signal_efficiency[2], background_rejection[2], label = \"All features\")\n",
    "\n",
    "    auc_high = np.trapz(background_rejection[0], signal_efficiency[0])\n",
    "    auc_low = np.trapz(background_rejection[1], signal_efficiency[1])\n",
    "    auc_all = np.trapz(background_rejection[2], signal_efficiency[2])\n",
    "\n",
    "    print(\"\\nAUC for high features = {}\\n\".format(auc_high))\n",
    "    print(\"\\nAUC for low features = {}\\n\".format(auc_low))\n",
    "    print(\"\\nAUC for all features = {}\\n\".format(auc_all))\n",
    "\n",
    "    plt.xlabel(\"Signal efficiency\")\n",
    "    plt.ylabel(\"Background rejection\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_significance(significance, p):\n",
    "    plt.plot(p, significance)\n",
    "    plt.xlabel(\"Probability of signal\")\n",
    "    plt.ylabel(\"Significance\")\n",
    "    plt.show()\n",
    "\n",
    "def testing(args):\n",
    "    args.validation = False\n",
    "    \n",
    "    train_loader, test_loader = loaders(args)\n",
    "    print(\"\\nTraining DNN with dataset size = {}, batch size = {}, learning rate = {}, momentum = {} and epochs = {}.\\n\".format(args.data_size, args.batch_size, args.lr, args.momentum, args.epochs))\n",
    "\n",
    "    test_probabilities = evaluate_model(args, train_loader, test_loader, 0)\n",
    "\n",
    "    plot_testing(test_probabilities,args.bins)\n",
    "\n",
    "    p = args.cut \n",
    "    \n",
    "    signal_r = []\n",
    "\n",
    "    for j in test_probabilities:\n",
    "        if j > p:\n",
    "            signal_r.append(j)\n",
    "\n",
    "    print(\"\\nNumber of signal events = {}\\n\".format(len(signal_r)))\n",
    "\n",
    "    plot_signal(test_probabilities, signal_r, args.bins)\n",
    "\n",
    "def plot_testing(test_probabilities, bins):\n",
    "    \n",
    "    #bins = int(np.sqrt(len(test_probabilities)))\n",
    "\n",
    "    plt.hist(test_probabilities, bins, alpha = 0.5)\n",
    "    #plt.vlines(x=0.88,y)\n",
    "    plt.xlabel(\"Probability of signal\")\n",
    "    plt.ylabel(\"Number of events\")\n",
    "    plt.title(\"Unlabeled data\")\n",
    "    #plt.legend(loc = \"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_signal(test_probabilities, signal_r, bins):\n",
    "\n",
    "    bins_signal = int(np.sqrt(len(signal_r)))\n",
    "\n",
    "    plt.hist(test_probabilities, bins, alpha = 0.5, label = \"Unlabeled data\")\n",
    "    plt.hist(signal_r, bins_signal, alpha = 0.5, label = \"Predicted Signal\")\n",
    "    plt.xlabel(\"Probability of signal\")\n",
    "    plt.ylabel(\"Number of events\")\n",
    "    plt.title(\"Signal prediction from unlabeled data\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Agora só nos resta definir os argumentos (args)\n",
    "parser = argparse.ArgumentParser(description='PyTorch HIGGS Example')\n",
    "parser.add_argument('--data_size', type=int, default=2000000, metavar='DS',\n",
    "                help='size of data set (default: 100000)')\n",
    "parser.add_argument('--high_feats', type=bool, default=True, metavar='HLF',         #True\n",
    "                help='toggles high level features (default: None)')\n",
    "parser.add_argument('--batch-size', type=int, default=int(0.01*2000000), metavar='N',\n",
    "                help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--epochs', type=int, default=30, metavar='N',                   #30\n",
    "                help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                help='learning rate (default: 0.02)')\n",
    "parser.add_argument('--momentum', type=float, default=0.8, metavar='M',\n",
    "                help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--seed', type=int, default=22, metavar='S',\n",
    "                help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--validation', type=bool, default=True, metavar='HLF',\n",
    "                help='i change this value in the functions just to be sure')\n",
    "parser.add_argument('--bins', type=int, default=int(np.sqrt(0.01*2000000)), metavar='HLF',\n",
    "                help='sqrt data set size')\n",
    "parser.add_argument('--cut', type=float, default=0.88, metavar='HLF',\n",
    "                help='cut that maximized significance in testing')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# set seed of random number generator\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#Otimização dos hyper-parametros:\n",
    "\n",
    "optim_datasize(args)\n",
    "#optim_lr(args)\n",
    "#optim_batch(args)\n",
    "#optim_momentum(args)\n",
    "#optim_epoch(args)\n",
    "\n",
    "\n",
    "#Validação do modelo\n",
    "#validation(args)\n",
    "\n",
    "#Teste de dados sem label\n",
    "#testing(args)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
